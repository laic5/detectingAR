{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.applications import VGG16, VGG19, ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalMaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from imagenet_utils import decode_predictions\n",
    "from imagenet_utils import preprocess_input\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ImageDataGenerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually want to use args and argparse to take in train_dir and test_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = 'train_CAL/'\n",
    "test_dir = 'test_CAL/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IM_HEIGHT = 224\n",
    "IM_WIDTH = 224\n",
    "NB_EPOCHS = 1\n",
    "BAT_SIZE = 16\n",
    "FC_SIZE = 500 # May need to train this parameter\n",
    "nb_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nb_files(directory):\n",
    "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "    cnt = 0\n",
    "    for r, dirs, files in os.walk(directory):\n",
    "        for dr in dirs:\n",
    "            cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = get_nb_files(train_dir)\n",
    "nb_classes = len(glob.glob(train_dir + \"/*\"))\n",
    "nb_val_samples = get_nb_files(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "    Args:\n",
    "    base_model: keras model excluding top\n",
    "    nb_classes: # of classes\n",
    "    Returns:\n",
    "    new keras model with last layer\n",
    "    \"\"\"\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x)\n",
    "    predictions = Dense(nb_classes, activation='sigmoid')(x) \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_to_transfer_learn(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        model.compile(optimizer='adam',    \n",
    "                    loss='binary_crossentropy', \n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore this for now\n",
    "def setup_to_finetune(model):\n",
    "    \"\"\"\n",
    "    If we want to fine-tine\n",
    "    \"\"\"\n",
    "    for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]: # need to declare num layers to train\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.0001, momentum=0.9),   \n",
    "                 loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to tune:\n",
    "* architecture type (ResNet, Inception, VGG19)\n",
    "* FC_SIZE\n",
    "* Adam parameters (momentum, learning rate, etc)\n",
    "* image data generator transforming parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_training(train_dir, test_dir, imheight=IM_HEIGHT, imwidth=IM_WIDTH, batch=BAT_SIZE):\n",
    "    train_datagen =  ImageDataGenerator(\n",
    "    #rotation_range=30,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    #rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        #rotation_range=30,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=BAT_SIZE\n",
    "    )\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=BAT_SIZE\n",
    "    )\n",
    "    \n",
    "    return (train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 362 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      " 12/720 [..............................] - ETA: 4:38:40 - loss: 6.3493 - acc: 0.40 - ETA: 4:15:12 - loss: 7.1603 - acc: 0.45 - ETA: 4:08:23 - loss: 7.4306 - acc: 0.46 - ETA: 3:59:18 - loss: 7.5657 - acc: 0.47 - ETA: 3:53:21 - loss: 7.6468 - acc: 0.48 - ETA: 3:51:09 - loss: 7.7009 - acc: 0.48 - ETA: 3:46:24 - loss: 7.7395 - acc: 0.48 - ETA: 3:45:36 - loss: 7.7685 - acc: 0.48 - ETA: 3:44:26 - loss: 7.7910 - acc: 0.48 - ETA: 3:35:43 - loss: 7.8090 - acc: 0.49 - ETA: 3:36:58 - loss: 7.8238 - acc: 0.49 - ETA: 3:36:37 - loss: 7.8360 - acc: 0.4922"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-37e29657b9a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-37e29657b9a4>\u001b[0m in \u001b[0;36mvgg_train\u001b[1;34m(train_dir, val_dir)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0msetup_to_transfer_learn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNB_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_train_samples\u001b[0m\u001b[1;33m,\u001b[0m                                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vgg_train.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def vgg16_train(train_dir, val_dir):\n",
    "    \n",
    "    train_gen, val_gen = prepare_training(train_dir=train_dir, test_dir=test_dir)\n",
    "\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    #base_model = VGG16(weights=None, include_top=False)\n",
    "    \n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "    history = model.fit_generator(train_gen, epochs = NB_EPOCHS, steps_per_epoch = nb_train_samples, \\\n",
    "                                     validation_data=val_gen, validation_steps = nb_val_samples)\n",
    "    model.save(\"vgg19.h5\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "h = vgg16_train(train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - ETA: 14:1 - ETA: 9:3 - ETA: 8: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 56 - ETA: 53 - ETA: 50 - ETA: 48 - ETA: 44 - ETA: 43 - ETA: 41 - ETA: 39 - ETA: 38 - ETA: 36 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 30 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 19 - ETA: 17 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 8s 0us/step\n",
      "Epoch 1/1\n",
      "194/720 [=======>......................] - ETA: 4:53:58 - loss: 10.2480 - acc: 0.250 - ETA: 4:40:20 - loss: 9.1206 - acc: 0.375 - ETA: 4:25:27 - loss: 7.4098 - acc: 0.44 - ETA: 4:18:21 - loss: 7.3825 - acc: 0.45 - ETA: 4:13:07 - loss: 6.9916 - acc: 0.45 - ETA: 4:10:18 - loss: 6.5340 - acc: 0.48 - ETA: 4:12:32 - loss: 5.9029 - acc: 0.53 - ETA: 4:10:24 - loss: 5.4155 - acc: 0.57 - ETA: 4:08:35 - loss: 5.3665 - acc: 0.58 - ETA: 4:06:58 - loss: 5.1305 - acc: 0.60 - ETA: 4:05:20 - loss: 4.8462 - acc: 0.63 - ETA: 4:03:54 - loss: 4.6093 - acc: 0.65 - ETA: 4:03:06 - loss: 4.5631 - acc: 0.66 - ETA: 4:02:37 - loss: 4.5949 - acc: 0.66 - ETA: 4:01:31 - loss: 4.5558 - acc: 0.66 - ETA: 4:00:45 - loss: 4.3963 - acc: 0.68 - ETA: 3:59:50 - loss: 4.3734 - acc: 0.68 - ETA: 4:00:01 - loss: 4.3531 - acc: 0.68 - ETA: 3:59:08 - loss: 4.2294 - acc: 0.69 - ETA: 3:58:15 - loss: 4.2183 - acc: 0.70 - ETA: 3:57:29 - loss: 4.1129 - acc: 0.70 - ETA: 3:57:42 - loss: 4.0626 - acc: 0.71 - ETA: 3:57:00 - loss: 4.1037 - acc: 0.71 - ETA: 3:56:29 - loss: 4.1415 - acc: 0.71 - ETA: 3:56:08 - loss: 4.2563 - acc: 0.70 - ETA: 3:55:35 - loss: 4.2853 - acc: 0.70 - ETA: 3:55:07 - loss: 4.3492 - acc: 0.70 - ETA: 3:55:27 - loss: 4.4444 - acc: 0.69 - ETA: 3:57:39 - loss: 4.4984 - acc: 0.69 - ETA: 3:57:20 - loss: 4.4153 - acc: 0.70 - ETA: 3:56:46 - loss: 4.3375 - acc: 0.70 - ETA: 3:57:40 - loss: 4.3585 - acc: 0.70 - ETA: 3:59:42 - loss: 4.4086 - acc: 0.70 - ETA: 4:01:00 - loss: 4.3378 - acc: 0.70 - ETA: 4:02:24 - loss: 4.3570 - acc: 0.70 - ETA: 4:03:12 - loss: 4.3195 - acc: 0.71 - ETA: 4:03:50 - loss: 4.4194 - acc: 0.70 - ETA: 4:03:53 - loss: 4.4349 - acc: 0.70 - ETA: 4:03:41 - loss: 4.4753 - acc: 0.70 - ETA: 4:02:54 - loss: 4.5137 - acc: 0.70 - ETA: 4:02:17 - loss: 4.4769 - acc: 0.70 - ETA: 4:01:37 - loss: 4.4419 - acc: 0.70 - ETA: 4:00:58 - loss: 4.3852 - acc: 0.71 - ETA: 4:00:23 - loss: 4.3311 - acc: 0.71 - ETA: 3:59:51 - loss: 4.2794 - acc: 0.71 - ETA: 3:59:16 - loss: 4.2299 - acc: 0.72 - ETA: 3:58:41 - loss: 4.1399 - acc: 0.72 - ETA: 3:58:09 - loss: 4.1371 - acc: 0.72 - ETA: 3:57:29 - loss: 4.1345 - acc: 0.72 - ETA: 3:56:47 - loss: 4.1119 - acc: 0.72 - ETA: 3:56:05 - loss: 4.1295 - acc: 0.72 - ETA: 3:55:26 - loss: 4.1657 - acc: 0.72 - ETA: 3:54:55 - loss: 4.1816 - acc: 0.72 - ETA: 3:54:14 - loss: 4.1784 - acc: 0.72 - ETA: 3:53:36 - loss: 4.2299 - acc: 0.72 - ETA: 3:52:57 - loss: 4.2618 - acc: 0.72 - ETA: 3:52:18 - loss: 4.2046 - acc: 0.72 - ETA: 3:51:41 - loss: 4.2012 - acc: 0.72 - ETA: 3:51:05 - loss: 4.1809 - acc: 0.72 - ETA: 3:50:33 - loss: 4.1947 - acc: 0.72 - ETA: 3:49:58 - loss: 4.1588 - acc: 0.72 - ETA: 3:49:23 - loss: 4.1564 - acc: 0.72 - ETA: 3:48:49 - loss: 4.1699 - acc: 0.72 - ETA: 3:48:15 - loss: 4.1987 - acc: 0.72 - ETA: 3:47:43 - loss: 4.2266 - acc: 0.72 - ETA: 3:47:20 - loss: 4.2232 - acc: 0.72 - ETA: 3:46:47 - loss: 4.2350 - acc: 0.72 - ETA: 3:46:20 - loss: 4.2022 - acc: 0.72 - ETA: 3:45:53 - loss: 4.1848 - acc: 0.72 - ETA: 3:45:29 - loss: 4.2109 - acc: 0.72 - ETA: 3:45:00 - loss: 4.2222 - acc: 0.72 - ETA: 3:44:38 - loss: 4.1913 - acc: 0.72 - ETA: 3:44:12 - loss: 4.2437 - acc: 0.72 - ETA: 3:43:48 - loss: 4.2270 - acc: 0.72 - ETA: 3:43:24 - loss: 4.2508 - acc: 0.72 - ETA: 3:43:01 - loss: 4.2608 - acc: 0.72 - ETA: 3:42:34 - loss: 4.2705 - acc: 0.72 - ETA: 3:42:10 - loss: 4.2543 - acc: 0.72 - ETA: 3:41:49 - loss: 4.2258 - acc: 0.72 - ETA: 3:41:20 - loss: 4.2105 - acc: 0.72 - ETA: 3:40:58 - loss: 4.2080 - acc: 0.72 - ETA: 3:40:38 - loss: 4.1934 - acc: 0.72 - ETA: 3:40:16 - loss: 4.2153 - acc: 0.72 - ETA: 3:39:55 - loss: 4.2128 - acc: 0.72 - ETA: 4:21:56 - loss: 4.1750 - acc: 0.73 - ETA: 4:21:00 - loss: 4.1731 - acc: 0.73 - ETA: 4:19:58 - loss: 4.1827 - acc: 0.73 - ETA: 4:18:58 - loss: 4.1807 - acc: 0.73 - ETA: 4:17:58 - loss: 4.1675 - acc: 0.73 - ETA: 4:17:00 - loss: 4.1546 - acc: 0.73 - ETA: 4:16:03 - loss: 4.1640 - acc: 0.73 - ETA: 4:15:05 - loss: 4.1514 - acc: 0.73 - ETA: 4:14:09 - loss: 4.1714 - acc: 0.73 - ETA: 4:13:13 - loss: 4.1803 - acc: 0.73 - ETA: 4:12:20 - loss: 4.1785 - acc: 0.73 - ETA: 4:11:28 - loss: 4.2185 - acc: 0.72 - ETA: 4:10:35 - loss: 4.2163 - acc: 0.72 - ETA: 4:09:41 - loss: 4.2142 - acc: 0.72 - ETA: 4:08:48 - loss: 4.1817 - acc: 0.73 - ETA: 4:07:56 - loss: 4.1900 - acc: 0.73 - ETA: 4:07:06 - loss: 4.1882 - acc: 0.73 - ETA: 4:06:37 - loss: 4.2061 - acc: 0.73 - ETA: 4:05:56 - loss: 4.2041 - acc: 0.73 - ETA: 4:05:12 - loss: 4.1830 - acc: 0.73 - ETA: 4:04:31 - loss: 4.1718 - acc: 0.73 - ETA: 4:03:53 - loss: 4.1702 - acc: 0.73 - ETA: 4:03:10 - loss: 4.1687 - acc: 0.73 - ETA: 4:02:29 - loss: 4.1672 - acc: 0.73 - ETA: 4:01:49 - loss: 4.1657 - acc: 0.73 - ETA: 4:01:22 - loss: 4.1916 - acc: 0.73 - ETA: 4:00:49 - loss: 4.1809 - acc: 0.73 - ETA: 4:00:23 - loss: 4.1794 - acc: 0.73 - ETA: 3:59:45 - loss: 4.1779 - acc: 0.73 - ETA: 3:59:10 - loss: 4.1588 - acc: 0.73 - ETA: 3:58:28 - loss: 4.1836 - acc: 0.73 - ETA: 3:57:47 - loss: 4.1735 - acc: 0.73 - ETA: 3:57:06 - loss: 4.1806 - acc: 0.73 - ETA: 3:56:24 - loss: 4.1791 - acc: 0.73 - ETA: 3:55:42 - loss: 4.1693 - acc: 0.73 - ETA: 3:55:00 - loss: 4.1679 - acc: 0.73 - ETA: 3:54:17 - loss: 4.1832 - acc: 0.73 - ETA: 3:53:35 - loss: 4.1817 - acc: 0.73 - ETA: 3:52:57 - loss: 4.1722 - acc: 0.73 - ETA: 3:52:17 - loss: 4.1628 - acc: 0.73 - ETA: 3:51:37 - loss: 4.1615 - acc: 0.73 - ETA: 3:51:00 - loss: 4.1523 - acc: 0.73 - ETA: 3:50:51 - loss: 4.1354 - acc: 0.73 - ETA: 3:51:01 - loss: 4.1266 - acc: 0.73 - ETA: 3:51:11 - loss: 4.1412 - acc: 0.73 - ETA: 3:51:17 - loss: 4.1402 - acc: 0.73 - ETA: 3:51:32 - loss: 4.1162 - acc: 0.73 - ETA: 3:51:40 - loss: 4.1154 - acc: 0.73 - ETA: 3:51:47 - loss: 4.1146 - acc: 0.73 - ETA: 3:51:54 - loss: 4.1063 - acc: 0.73 - ETA: 3:52:02 - loss: 4.1130 - acc: 0.73 - ETA: 3:52:10 - loss: 4.1196 - acc: 0.73 - ETA: 3:52:17 - loss: 4.1261 - acc: 0.73 - ETA: 3:52:23 - loss: 4.1325 - acc: 0.73 - ETA: 3:52:28 - loss: 4.1460 - acc: 0.73 - ETA: 3:52:33 - loss: 4.1450 - acc: 0.73 - ETA: 3:52:37 - loss: 4.1440 - acc: 0.73 - ETA: 3:52:39 - loss: 4.1642 - acc: 0.73 - ETA: 3:52:49 - loss: 4.1702 - acc: 0.73 - ETA: 3:52:51 - loss: 4.1760 - acc: 0.73 - ETA: 3:52:54 - loss: 4.1610 - acc: 0.73 - ETA: 3:52:53 - loss: 4.1600 - acc: 0.73 - ETA: 3:52:57 - loss: 4.1794 - acc: 0.73 - ETA: 3:52:59 - loss: 4.1782 - acc: 0.73 - ETA: 3:52:58 - loss: 4.1636 - acc: 0.73 - ETA: 3:53:01 - loss: 4.1626 - acc: 0.73 - ETA: 3:52:58 - loss: 4.1549 - acc: 0.73 - ETA: 3:52:56 - loss: 4.1473 - acc: 0.73 - ETA: 3:52:52 - loss: 4.1333 - acc: 0.73 - ETA: 3:52:49 - loss: 4.1390 - acc: 0.73 - ETA: 3:52:48 - loss: 4.1253 - acc: 0.73 - ETA: 3:52:42 - loss: 4.1373 - acc: 0.73 - ETA: 3:52:38 - loss: 4.1301 - acc: 0.73 - ETA: 3:52:33 - loss: 4.1357 - acc: 0.73 - ETA: 3:52:32 - loss: 4.1349 - acc: 0.73 - ETA: 3:52:25 - loss: 4.1278 - acc: 0.73 - ETA: 3:52:20 - loss: 4.1271 - acc: 0.73 - ETA: 3:52:15 - loss: 4.1264 - acc: 0.73 - ETA: 3:52:14 - loss: 4.1133 - acc: 0.73 - ETA: 3:52:07 - loss: 4.1005 - acc: 0.73 - ETA: 3:52:02 - loss: 4.0999 - acc: 0.74 - ETA: 3:51:56 - loss: 4.0993 - acc: 0.74 - ETA: 3:51:48 - loss: 4.0928 - acc: 0.74 - ETA: 3:51:40 - loss: 4.0923 - acc: 0.74 - ETA: 3:51:32 - loss: 4.1036 - acc: 0.73 - ETA: 3:51:28 - loss: 4.1208 - acc: 0.73 - ETA: 3:51:19 - loss: 4.1201 - acc: 0.73 - ETA: 3:51:11 - loss: 4.1253 - acc: 0.73 - ETA: 3:51:00 - loss: 4.1188 - acc: 0.73 - ETA: 3:50:36 - loss: 4.1239 - acc: 0.73 - ETA: 3:50:08 - loss: 4.1175 - acc: 0.73 - ETA: 3:49:35 - loss: 4.1169 - acc: 0.73 - ETA: 3:48:53 - loss: 4.1050 - acc: 0.73 - ETA: 3:48:11 - loss: 4.1044 - acc: 0.74 - ETA: 3:47:29 - loss: 4.0927 - acc: 0.74 - ETA: 3:46:47 - loss: 4.0922 - acc: 0.74 - ETA: 3:46:05 - loss: 4.0973 - acc: 0.74 - ETA: 3:45:23 - loss: 4.0913 - acc: 0.74 - ETA: 3:44:43 - loss: 4.0908 - acc: 0.74 - ETA: 3:44:03 - loss: 4.0795 - acc: 0.74 - ETA: 3:43:23 - loss: 4.0845 - acc: 0.74 - ETA: 3:42:44 - loss: 4.0733 - acc: 0.74 - ETA: 3:42:08 - loss: 4.0730 - acc: 0.74 - ETA: 3:41:28 - loss: 4.0726 - acc: 0.74 - ETA: 3:40:49 - loss: 4.0670 - acc: 0.74 - ETA: 3:40:09 - loss: 4.0667 - acc: 0.74 - ETA: 3:39:31 - loss: 4.0506 - acc: 0.74 - ETA: 3:38:51 - loss: 4.0608 - acc: 0.74 - ETA: 3:38:12 - loss: 4.0554 - acc: 0.74 - ETA: 3:37:34 - loss: 4.0603 - acc: 0.7431389/720 [===============>..............] - ETA: 3:36:55 - loss: 4.0549 - acc: 0.74 - ETA: 3:36:17 - loss: 4.0495 - acc: 0.74 - ETA: 3:35:39 - loss: 4.0646 - acc: 0.74 - ETA: 3:35:00 - loss: 4.0693 - acc: 0.74 - ETA: 3:34:23 - loss: 4.0590 - acc: 0.74 - ETA: 3:33:48 - loss: 4.0537 - acc: 0.74 - ETA: 3:33:12 - loss: 4.0634 - acc: 0.74 - ETA: 3:32:38 - loss: 4.0830 - acc: 0.74 - ETA: 3:32:04 - loss: 4.0826 - acc: 0.74 - ETA: 3:31:29 - loss: 4.0822 - acc: 0.74 - ETA: 3:30:56 - loss: 4.1014 - acc: 0.74 - ETA: 3:30:19 - loss: 4.1107 - acc: 0.74 - ETA: 3:29:44 - loss: 4.1199 - acc: 0.73 - ETA: 3:29:07 - loss: 4.1097 - acc: 0.74 - ETA: 3:28:32 - loss: 4.0948 - acc: 0.74 - ETA: 3:27:58 - loss: 4.0801 - acc: 0.74 - ETA: 3:27:25 - loss: 4.0845 - acc: 0.74 - ETA: 3:26:52 - loss: 4.0842 - acc: 0.74 - ETA: 3:26:18 - loss: 4.0744 - acc: 0.74 - ETA: 3:25:46 - loss: 4.0600 - acc: 0.74 - ETA: 3:25:12 - loss: 4.0598 - acc: 0.74 - ETA: 3:24:40 - loss: 4.0642 - acc: 0.74 - ETA: 3:24:06 - loss: 4.0593 - acc: 0.74 - ETA: 3:23:33 - loss: 4.0591 - acc: 0.74 - ETA: 3:23:00 - loss: 4.0588 - acc: 0.74 - ETA: 3:22:27 - loss: 4.0677 - acc: 0.74 - ETA: 3:21:54 - loss: 4.0629 - acc: 0.74 - ETA: 3:21:20 - loss: 4.0762 - acc: 0.74 - ETA: 3:20:48 - loss: 4.0804 - acc: 0.74 - ETA: 3:20:15 - loss: 4.0845 - acc: 0.74 - ETA: 3:19:43 - loss: 4.0797 - acc: 0.74 - ETA: 3:19:11 - loss: 4.0750 - acc: 0.74 - ETA: 3:18:37 - loss: 4.0747 - acc: 0.74 - ETA: 3:18:05 - loss: 4.0744 - acc: 0.74 - ETA: 3:17:33 - loss: 4.0653 - acc: 0.74 - ETA: 3:17:01 - loss: 4.0607 - acc: 0.74 - ETA: 3:16:28 - loss: 4.0518 - acc: 0.74 - ETA: 3:15:57 - loss: 4.0516 - acc: 0.74 - ETA: 3:15:25 - loss: 4.0644 - acc: 0.74 - ETA: 3:14:54 - loss: 4.0727 - acc: 0.74 - ETA: 3:14:23 - loss: 4.0724 - acc: 0.74 - ETA: 3:13:52 - loss: 4.0721 - acc: 0.74 - ETA: 3:13:21 - loss: 4.0676 - acc: 0.74 - ETA: 3:12:51 - loss: 4.0716 - acc: 0.74 - ETA: 3:12:21 - loss: 4.0755 - acc: 0.74 - ETA: 3:11:51 - loss: 4.0794 - acc: 0.74 - ETA: 3:11:21 - loss: 4.0708 - acc: 0.74 - ETA: 3:10:52 - loss: 4.0788 - acc: 0.74 - ETA: 3:10:22 - loss: 4.0661 - acc: 0.74 - ETA: 3:09:53 - loss: 4.0536 - acc: 0.74 - ETA: 3:09:23 - loss: 4.0493 - acc: 0.74 - ETA: 3:38:10 - loss: 4.0491 - acc: 0.74 - ETA: 3:37:30 - loss: 4.0490 - acc: 0.74 - ETA: 3:36:49 - loss: 4.0569 - acc: 0.74 - ETA: 3:36:21 - loss: 4.0728 - acc: 0.74 - ETA: 3:35:50 - loss: 4.0725 - acc: 0.74 - ETA: 3:35:13 - loss: 4.0762 - acc: 0.74 - ETA: 3:34:35 - loss: 4.0839 - acc: 0.74 - ETA: 3:33:57 - loss: 4.0876 - acc: 0.74 - ETA: 3:33:16 - loss: 4.0952 - acc: 0.74 - ETA: 3:32:36 - loss: 4.0987 - acc: 0.74 - ETA: 3:31:57 - loss: 4.0906 - acc: 0.74 - ETA: 3:31:24 - loss: 4.0824 - acc: 0.74 - ETA: 3:30:43 - loss: 4.0899 - acc: 0.74 - ETA: 3:30:02 - loss: 4.0857 - acc: 0.74 - ETA: 3:29:21 - loss: 4.0816 - acc: 0.74 - ETA: 3:28:48 - loss: 4.0813 - acc: 0.74 - ETA: 3:28:16 - loss: 4.0810 - acc: 0.74 - ETA: 3:27:43 - loss: 4.0693 - acc: 0.74 - ETA: 3:27:11 - loss: 4.0691 - acc: 0.74 - ETA: 3:26:39 - loss: 4.0651 - acc: 0.74 - ETA: 3:26:05 - loss: 4.0498 - acc: 0.74 - ETA: 3:25:35 - loss: 4.0684 - acc: 0.74 - ETA: 3:25:04 - loss: 4.0682 - acc: 0.74 - ETA: 3:24:27 - loss: 4.0642 - acc: 0.74 - ETA: 3:23:55 - loss: 4.0714 - acc: 0.74 - ETA: 3:23:21 - loss: 4.0675 - acc: 0.74 - ETA: 3:22:48 - loss: 4.0746 - acc: 0.74 - ETA: 3:22:11 - loss: 4.0781 - acc: 0.74 - ETA: 3:21:35 - loss: 4.0851 - acc: 0.74 - ETA: 3:21:02 - loss: 4.0775 - acc: 0.74 - ETA: 3:20:24 - loss: 4.0773 - acc: 0.74 - ETA: 3:19:52 - loss: 4.0698 - acc: 0.74 - ETA: 3:19:16 - loss: 4.0732 - acc: 0.74 - ETA: 3:18:41 - loss: 4.0694 - acc: 0.74 - ETA: 3:18:21 - loss: 4.0763 - acc: 0.74 - ETA: 3:20:35 - loss: 4.0796 - acc: 0.74 - ETA: 3:23:03 - loss: 4.0829 - acc: 0.74 - ETA: 3:25:06 - loss: 4.0862 - acc: 0.74 - ETA: 8:03:53 - loss: 4.0859 - acc: 0.74 - ETA: 8:01:46 - loss: 4.0821 - acc: 0.74 - ETA: 7:59:34 - loss: 4.0784 - acc: 0.74 - ETA: 7:57:24 - loss: 4.0711 - acc: 0.74 - ETA: 7:55:19 - loss: 4.0709 - acc: 0.74 - ETA: 7:53:08 - loss: 4.0707 - acc: 0.74 - ETA: 7:50:54 - loss: 4.0739 - acc: 0.74 - ETA: 7:48:41 - loss: 4.0771 - acc: 0.74 - ETA: 7:46:29 - loss: 4.0735 - acc: 0.74 - ETA: 7:44:17 - loss: 4.0698 - acc: 0.74 - ETA: 7:42:07 - loss: 4.0696 - acc: 0.74 - ETA: 7:39:56 - loss: 4.0830 - acc: 0.74 - ETA: 7:37:48 - loss: 4.0726 - acc: 0.74 - ETA: 7:35:45 - loss: 4.0724 - acc: 0.74 - ETA: 7:33:39 - loss: 4.0721 - acc: 0.74 - ETA: 7:31:36 - loss: 4.0786 - acc: 0.74 - ETA: 7:29:36 - loss: 4.0750 - acc: 0.74 - ETA: 7:27:32 - loss: 4.0648 - acc: 0.74 - ETA: 7:25:29 - loss: 4.0613 - acc: 0.74 - ETA: 7:23:27 - loss: 4.0678 - acc: 0.74 - ETA: 7:21:24 - loss: 4.0643 - acc: 0.74 - ETA: 7:19:25 - loss: 4.0772 - acc: 0.74 - ETA: 7:17:30 - loss: 4.0704 - acc: 0.74 - ETA: 7:15:31 - loss: 4.0637 - acc: 0.74 - ETA: 7:13:31 - loss: 4.0603 - acc: 0.74 - ETA: 7:11:33 - loss: 4.0536 - acc: 0.74 - ETA: 7:09:35 - loss: 4.0535 - acc: 0.74 - ETA: 7:07:37 - loss: 4.0501 - acc: 0.74 - ETA: 7:05:43 - loss: 4.0500 - acc: 0.74 - ETA: 7:03:47 - loss: 4.0562 - acc: 0.74 - ETA: 7:01:51 - loss: 4.0625 - acc: 0.74 - ETA: 6:59:55 - loss: 4.0655 - acc: 0.74 - ETA: 6:57:58 - loss: 4.0558 - acc: 0.74 - ETA: 6:56:04 - loss: 4.0651 - acc: 0.74 - ETA: 6:54:10 - loss: 4.0649 - acc: 0.74 - ETA: 6:52:17 - loss: 4.0616 - acc: 0.74 - ETA: 6:50:25 - loss: 4.0614 - acc: 0.74 - ETA: 6:48:34 - loss: 4.0675 - acc: 0.74 - ETA: 6:46:44 - loss: 4.0704 - acc: 0.74 - ETA: 6:44:54 - loss: 4.0578 - acc: 0.74 - ETA: 6:43:03 - loss: 4.0639 - acc: 0.74 - ETA: 6:41:13 - loss: 4.0699 - acc: 0.74 - ETA: 6:39:23 - loss: 4.0697 - acc: 0.74 - ETA: 6:37:34 - loss: 4.0664 - acc: 0.74 - ETA: 6:35:46 - loss: 4.0662 - acc: 0.74 - ETA: 6:33:57 - loss: 4.0691 - acc: 0.74 - ETA: 6:32:09 - loss: 4.0689 - acc: 0.74 - ETA: 6:30:22 - loss: 4.0687 - acc: 0.74 - ETA: 6:28:35 - loss: 4.0685 - acc: 0.74 - ETA: 6:26:49 - loss: 4.0623 - acc: 0.74 - ETA: 6:25:04 - loss: 4.0592 - acc: 0.74 - ETA: 6:23:19 - loss: 4.0560 - acc: 0.74 - ETA: 6:21:34 - loss: 4.0619 - acc: 0.74 - ETA: 6:19:51 - loss: 4.0528 - acc: 0.74 - ETA: 6:18:09 - loss: 4.0497 - acc: 0.74 - ETA: 6:16:26 - loss: 4.0525 - acc: 0.74 - ETA: 6:14:45 - loss: 4.0494 - acc: 0.74 - ETA: 6:13:03 - loss: 4.0522 - acc: 0.74 - ETA: 6:11:21 - loss: 4.0463 - acc: 0.74 - ETA: 6:09:40 - loss: 4.0432 - acc: 0.74 - ETA: 6:08:00 - loss: 4.0431 - acc: 0.74 - ETA: 6:06:19 - loss: 4.0430 - acc: 0.74 - ETA: 6:04:41 - loss: 4.0487 - acc: 0.74 - ETA: 6:03:01 - loss: 4.0515 - acc: 0.74 - ETA: 6:01:22 - loss: 4.0485 - acc: 0.74 - ETA: 5:59:44 - loss: 4.0483 - acc: 0.74 - ETA: 5:58:08 - loss: 4.0511 - acc: 0.74 - ETA: 5:56:31 - loss: 4.0538 - acc: 0.74 - ETA: 5:54:55 - loss: 4.0508 - acc: 0.74 - ETA: 5:53:18 - loss: 4.0592 - acc: 0.74 - ETA: 5:51:43 - loss: 4.0619 - acc: 0.74 - ETA: 5:50:07 - loss: 4.0533 - acc: 0.74 - ETA: 5:48:31 - loss: 4.0560 - acc: 0.74 - ETA: 5:46:55 - loss: 4.0559 - acc: 0.74 - ETA: 5:45:20 - loss: 4.0557 - acc: 0.74 - ETA: 5:43:45 - loss: 4.0584 - acc: 0.74 - ETA: 5:42:11 - loss: 4.0610 - acc: 0.74 - ETA: 5:40:40 - loss: 4.0553 - acc: 0.74 - ETA: 5:39:13 - loss: 4.0524 - acc: 0.74 - ETA: 5:37:44 - loss: 4.0495 - acc: 0.74 - ETA: 5:36:11 - loss: 4.0494 - acc: 0.74 - ETA: 5:34:40 - loss: 4.0438 - acc: 0.74 - ETA: 5:33:10 - loss: 4.0410 - acc: 0.74 - ETA: 5:31:39 - loss: 4.0463 - acc: 0.74 - ETA: 5:30:18 - loss: 4.0490 - acc: 0.74 - ETA: 5:28:55 - loss: 4.0516 - acc: 0.74 - ETA: 5:27:30 - loss: 4.0460 - acc: 0.74 - ETA: 5:26:06 - loss: 4.0432 - acc: 0.74 - ETA: 5:24:41 - loss: 4.0431 - acc: 0.74 - ETA: 5:23:16 - loss: 4.0404 - acc: 0.74 - ETA: 5:21:52 - loss: 4.0429 - acc: 0.74 - ETA: 5:20:28 - loss: 4.0455 - acc: 0.74 - ETA: 5:19:04 - loss: 4.0534 - acc: 0.74 - ETA: 5:17:41 - loss: 4.0506 - acc: 0.74 - ETA: 5:16:19 - loss: 4.0505 - acc: 0.74 - ETA: 5:14:57 - loss: 4.0504 - acc: 0.74 - ETA: 5:13:32 - loss: 4.0450 - acc: 0.74 - ETA: 5:12:07 - loss: 4.0502 - acc: 0.74 - ETA: 5:10:44 - loss: 4.0553 - acc: 0.74 - ETA: 5:09:28 - loss: 4.0500 - acc: 0.74 - ETA: 5:08:11 - loss: 4.0498 - acc: 0.74 - ETA: 5:06:51 - loss: 4.0497 - acc: 0.74 - ETA: 5:05:30 - loss: 4.0496 - acc: 0.74 - ETA: 5:04:11 - loss: 4.0443 - acc: 0.74 - ETA: 5:02:50 - loss: 4.0468 - acc: 0.74 - ETA: 5:01:29 - loss: 4.0545 - acc: 0.7453584/720 [=======================>......] - ETA: 5:00:12 - loss: 4.0543 - acc: 0.74 - ETA: 4:58:52 - loss: 4.0568 - acc: 0.74 - ETA: 4:57:33 - loss: 4.0490 - acc: 0.74 - ETA: 4:56:15 - loss: 4.0540 - acc: 0.74 - ETA: 4:54:57 - loss: 4.0513 - acc: 0.74 - ETA: 4:53:38 - loss: 4.0487 - acc: 0.74 - ETA: 4:52:21 - loss: 4.0536 - acc: 0.74 - ETA: 4:51:03 - loss: 4.0535 - acc: 0.74 - ETA: 4:49:45 - loss: 4.0509 - acc: 0.74 - ETA: 4:48:28 - loss: 4.0508 - acc: 0.74 - ETA: 4:47:10 - loss: 4.0482 - acc: 0.74 - ETA: 4:45:53 - loss: 4.0531 - acc: 0.74 - ETA: 4:44:36 - loss: 4.0529 - acc: 0.74 - ETA: 4:43:20 - loss: 4.0503 - acc: 0.74 - ETA: 4:42:01 - loss: 4.0552 - acc: 0.74 - ETA: 4:40:41 - loss: 4.0576 - acc: 0.74 - ETA: 4:39:23 - loss: 4.0550 - acc: 0.74 - ETA: 4:38:04 - loss: 4.0573 - acc: 0.74 - ETA: 4:36:46 - loss: 4.0547 - acc: 0.74 - ETA: 4:35:29 - loss: 4.0522 - acc: 0.74 - ETA: 4:34:11 - loss: 4.0521 - acc: 0.74 - ETA: 4:32:53 - loss: 4.0593 - acc: 0.74 - ETA: 4:31:36 - loss: 4.0616 - acc: 0.74 - ETA: 4:30:21 - loss: 4.0614 - acc: 0.74 - ETA: 4:29:05 - loss: 4.0637 - acc: 0.74 - ETA: 4:27:50 - loss: 4.0612 - acc: 0.74 - ETA: 4:26:34 - loss: 4.0586 - acc: 0.74 - ETA: 4:25:19 - loss: 4.0609 - acc: 0.74 - ETA: 4:24:04 - loss: 4.0608 - acc: 0.74 - ETA: 4:22:49 - loss: 4.0559 - acc: 0.74 - ETA: 4:21:35 - loss: 4.0558 - acc: 0.74 - ETA: 4:20:21 - loss: 4.0557 - acc: 0.74 - ETA: 4:19:08 - loss: 4.0532 - acc: 0.74 - ETA: 4:17:54 - loss: 4.0578 - acc: 0.74 - ETA: 4:16:41 - loss: 4.0600 - acc: 0.74 - ETA: 4:15:27 - loss: 4.0599 - acc: 0.74 - ETA: 4:14:15 - loss: 4.0527 - acc: 0.74 - ETA: 4:13:01 - loss: 4.0503 - acc: 0.74 - ETA: 4:11:49 - loss: 4.0455 - acc: 0.74 - ETA: 4:10:35 - loss: 4.0431 - acc: 0.74 - ETA: 4:09:23 - loss: 4.0477 - acc: 0.74 - ETA: 4:08:10 - loss: 4.0499 - acc: 0.74 - ETA: 4:06:58 - loss: 4.0544 - acc: 0.74 - ETA: 4:05:46 - loss: 4.0543 - acc: 0.74 - ETA: 4:04:36 - loss: 4.0565 - acc: 0.74 - ETA: 4:03:25 - loss: 4.0610 - acc: 0.74 - ETA: 4:02:14 - loss: 4.0563 - acc: 0.74 - ETA: 4:01:05 - loss: 4.0493 - acc: 0.74 - ETA: 3:59:54 - loss: 4.0492 - acc: 0.74 - ETA: 3:58:44 - loss: 4.0445 - acc: 0.74 - ETA: 3:57:34 - loss: 4.0467 - acc: 0.74 - ETA: 3:56:24 - loss: 4.0512 - acc: 0.74 - ETA: 3:55:14 - loss: 4.0443 - acc: 0.74 - ETA: 3:54:05 - loss: 4.0487 - acc: 0.74 - ETA: 3:52:56 - loss: 4.0441 - acc: 0.74 - ETA: 3:51:47 - loss: 4.0418 - acc: 0.74 - ETA: 3:50:39 - loss: 4.0462 - acc: 0.74 - ETA: 3:49:32 - loss: 4.0461 - acc: 0.74 - ETA: 3:48:24 - loss: 4.0483 - acc: 0.74 - ETA: 3:47:16 - loss: 4.0527 - acc: 0.74 - ETA: 3:46:08 - loss: 4.0548 - acc: 0.74 - ETA: 3:45:02 - loss: 4.0502 - acc: 0.74 - ETA: 3:43:55 - loss: 4.0546 - acc: 0.74 - ETA: 3:42:50 - loss: 4.0545 - acc: 0.74 - ETA: 3:41:44 - loss: 4.0499 - acc: 0.74 - ETA: 3:40:37 - loss: 4.0521 - acc: 0.74 - ETA: 3:39:32 - loss: 4.0520 - acc: 0.74 - ETA: 3:38:26 - loss: 4.0541 - acc: 0.74 - ETA: 3:37:20 - loss: 4.0518 - acc: 0.74 - ETA: 3:36:14 - loss: 4.0517 - acc: 0.74 - ETA: 3:35:09 - loss: 4.0472 - acc: 0.74 - ETA: 3:34:03 - loss: 4.0428 - acc: 0.74 - ETA: 3:32:57 - loss: 4.0362 - acc: 0.74 - ETA: 3:31:52 - loss: 4.0426 - acc: 0.74 - ETA: 3:30:48 - loss: 4.0382 - acc: 0.74 - ETA: 3:29:44 - loss: 4.0425 - acc: 0.74 - ETA: 3:28:39 - loss: 4.0446 - acc: 0.74 - ETA: 3:27:35 - loss: 4.0488 - acc: 0.74 - ETA: 3:26:31 - loss: 4.0487 - acc: 0.74 - ETA: 3:25:26 - loss: 4.0443 - acc: 0.74 - ETA: 3:24:23 - loss: 4.0442 - acc: 0.74 - ETA: 3:23:19 - loss: 4.0442 - acc: 0.74 - ETA: 3:22:15 - loss: 4.0420 - acc: 0.74 - ETA: 3:21:12 - loss: 4.0398 - acc: 0.74 - ETA: 3:20:08 - loss: 4.0439 - acc: 0.74 - ETA: 3:19:05 - loss: 4.0460 - acc: 0.74 - ETA: 3:18:03 - loss: 4.0438 - acc: 0.74 - ETA: 3:17:00 - loss: 4.0458 - acc: 0.74 - ETA: 3:15:58 - loss: 4.0478 - acc: 0.74 - ETA: 3:14:57 - loss: 4.0561 - acc: 0.74 - ETA: 3:13:56 - loss: 4.0560 - acc: 0.74 - ETA: 3:12:54 - loss: 4.0517 - acc: 0.74 - ETA: 3:11:53 - loss: 4.0516 - acc: 0.74 - ETA: 3:10:51 - loss: 4.0516 - acc: 0.74 - ETA: 3:09:49 - loss: 4.0577 - acc: 0.74 - ETA: 3:08:49 - loss: 4.0555 - acc: 0.74 - ETA: 3:07:50 - loss: 4.0575 - acc: 0.74 - ETA: 3:06:49 - loss: 4.0574 - acc: 0.74 - ETA: 3:05:50 - loss: 4.0573 - acc: 0.74 - ETA: 3:04:50 - loss: 4.0613 - acc: 0.74 - ETA: 3:03:49 - loss: 4.0550 - acc: 0.74 - ETA: 3:02:48 - loss: 4.0570 - acc: 0.74 - ETA: 3:01:48 - loss: 4.0528 - acc: 0.74 - ETA: 3:00:48 - loss: 4.0547 - acc: 0.74 - ETA: 2:59:49 - loss: 4.0546 - acc: 0.74 - ETA: 2:58:49 - loss: 4.0525 - acc: 0.74 - ETA: 2:57:50 - loss: 4.0524 - acc: 0.74 - ETA: 2:56:51 - loss: 4.0503 - acc: 0.74 - ETA: 2:55:52 - loss: 4.0542 - acc: 0.74 - ETA: 2:54:53 - loss: 4.0622 - acc: 0.74 - ETA: 2:53:54 - loss: 4.0561 - acc: 0.74 - ETA: 2:52:54 - loss: 4.0600 - acc: 0.74 - ETA: 2:51:56 - loss: 4.0579 - acc: 0.74 - ETA: 2:50:58 - loss: 4.0598 - acc: 0.74 - ETA: 2:50:00 - loss: 4.0557 - acc: 0.74 - ETA: 2:49:01 - loss: 4.0496 - acc: 0.74 - ETA: 2:48:04 - loss: 4.0476 - acc: 0.74 - ETA: 2:47:06 - loss: 4.0475 - acc: 0.74 - ETA: 2:46:08 - loss: 4.0435 - acc: 0.74 - ETA: 2:45:11 - loss: 4.0454 - acc: 0.74 - ETA: 2:44:14 - loss: 4.0492 - acc: 0.74 - ETA: 2:43:17 - loss: 4.0570 - acc: 0.74 - ETA: 2:42:20 - loss: 4.0510 - acc: 0.74 - ETA: 2:41:23 - loss: 4.0490 - acc: 0.74 - ETA: 2:40:26 - loss: 4.0508 - acc: 0.74 - ETA: 2:39:28 - loss: 4.0488 - acc: 0.74 - ETA: 2:38:32 - loss: 4.0507 - acc: 0.74 - ETA: 2:37:35 - loss: 4.0467 - acc: 0.74 - ETA: 2:36:39 - loss: 4.0447 - acc: 0.74 - ETA: 2:35:43 - loss: 4.0427 - acc: 0.74 - ETA: 2:34:48 - loss: 4.0426 - acc: 0.74 - ETA: 2:33:53 - loss: 4.0426 - acc: 0.74 - ETA: 2:32:58 - loss: 4.0463 - acc: 0.74 - ETA: 2:32:02 - loss: 4.0463 - acc: 0.74 - ETA: 2:31:07 - loss: 4.0462 - acc: 0.74 - ETA: 2:30:12 - loss: 4.0461 - acc: 0.74 - ETA: 2:29:17 - loss: 4.0441 - acc: 0.74 - ETA: 2:28:23 - loss: 4.0479 - acc: 0.74 - ETA: 2:27:28 - loss: 4.0459 - acc: 0.74 - ETA: 2:26:33 - loss: 4.0496 - acc: 0.74 - ETA: 2:25:38 - loss: 4.0514 - acc: 0.74 - ETA: 2:24:43 - loss: 4.0513 - acc: 0.74 - ETA: 2:23:49 - loss: 4.0513 - acc: 0.74 - ETA: 2:22:56 - loss: 4.0512 - acc: 0.74 - ETA: 2:22:02 - loss: 4.0511 - acc: 0.74 - ETA: 2:21:08 - loss: 4.0510 - acc: 0.74 - ETA: 2:20:14 - loss: 4.0491 - acc: 0.74 - ETA: 2:19:21 - loss: 4.0490 - acc: 0.74 - ETA: 2:18:27 - loss: 4.0452 - acc: 0.74 - ETA: 2:17:34 - loss: 4.0488 - acc: 0.74 - ETA: 2:16:41 - loss: 4.0506 - acc: 0.74 - ETA: 2:15:48 - loss: 4.0524 - acc: 0.74 - ETA: 2:14:55 - loss: 4.0523 - acc: 0.74 - ETA: 2:14:01 - loss: 4.0559 - acc: 0.74 - ETA: 2:13:08 - loss: 4.0503 - acc: 0.74 - ETA: 2:12:15 - loss: 4.0521 - acc: 0.74 - ETA: 2:11:21 - loss: 4.0538 - acc: 0.74 - ETA: 2:10:29 - loss: 4.0556 - acc: 0.74 - ETA: 2:09:36 - loss: 4.0555 - acc: 0.74 - ETA: 2:08:43 - loss: 4.0590 - acc: 0.74 - ETA: 2:07:50 - loss: 4.0553 - acc: 0.74 - ETA: 2:06:58 - loss: 4.0516 - acc: 0.74 - ETA: 2:06:05 - loss: 4.0551 - acc: 0.74 - ETA: 2:05:14 - loss: 4.0550 - acc: 0.74 - ETA: 2:04:21 - loss: 4.0531 - acc: 0.74 - ETA: 2:03:29 - loss: 4.0549 - acc: 0.74 - ETA: 2:02:37 - loss: 4.0566 - acc: 0.74 - ETA: 2:01:46 - loss: 4.0547 - acc: 0.74 - ETA: 2:00:54 - loss: 4.0510 - acc: 0.74 - ETA: 2:00:03 - loss: 4.0456 - acc: 0.74 - ETA: 1:59:11 - loss: 4.0455 - acc: 0.74 - ETA: 1:58:20 - loss: 4.0419 - acc: 0.74 - ETA: 1:57:29 - loss: 4.0472 - acc: 0.74 - ETA: 1:56:38 - loss: 4.0489 - acc: 0.74 - ETA: 1:55:47 - loss: 4.0506 - acc: 0.74 - ETA: 1:54:56 - loss: 4.0505 - acc: 0.74 - ETA: 1:54:05 - loss: 4.0504 - acc: 0.74 - ETA: 1:53:15 - loss: 4.0450 - acc: 0.74 - ETA: 1:52:24 - loss: 4.0485 - acc: 0.74 - ETA: 1:51:34 - loss: 4.0431 - acc: 0.74 - ETA: 1:50:43 - loss: 4.0448 - acc: 0.74 - ETA: 1:49:53 - loss: 4.0395 - acc: 0.74 - ETA: 1:49:03 - loss: 4.0465 - acc: 0.74 - ETA: 1:48:13 - loss: 4.0429 - acc: 0.74 - ETA: 1:47:23 - loss: 4.0463 - acc: 0.74 - ETA: 1:46:34 - loss: 4.0497 - acc: 0.74 - ETA: 1:45:45 - loss: 4.0462 - acc: 0.74 - ETA: 1:44:55 - loss: 4.0461 - acc: 0.74 - ETA: 1:44:06 - loss: 4.0409 - acc: 0.74 - ETA: 1:43:17 - loss: 4.0460 - acc: 0.74 - ETA: 1:42:28 - loss: 4.0459 - acc: 0.74 - ETA: 1:41:38 - loss: 4.0510 - acc: 0.74 - ETA: 1:40:49 - loss: 4.0458 - acc: 0.74 - ETA: 1:40:00 - loss: 4.0509 - acc: 0.74 - ETA: 1:39:11 - loss: 4.0508 - acc: 0.7461720/720 [==============================] - ETA: 1:38:22 - loss: 4.0490 - acc: 0.74 - ETA: 1:37:34 - loss: 4.0489 - acc: 0.74 - ETA: 1:36:45 - loss: 4.0523 - acc: 0.74 - ETA: 1:35:57 - loss: 4.0488 - acc: 0.74 - ETA: 1:35:08 - loss: 4.0470 - acc: 0.74 - ETA: 1:34:20 - loss: 4.0470 - acc: 0.74 - ETA: 1:33:32 - loss: 4.0452 - acc: 0.74 - ETA: 1:32:44 - loss: 4.0485 - acc: 0.74 - ETA: 1:31:56 - loss: 4.0468 - acc: 0.74 - ETA: 1:31:08 - loss: 4.0484 - acc: 0.74 - ETA: 1:30:20 - loss: 4.0466 - acc: 0.74 - ETA: 1:29:32 - loss: 4.0432 - acc: 0.74 - ETA: 1:28:45 - loss: 4.0381 - acc: 0.74 - ETA: 1:27:57 - loss: 4.0448 - acc: 0.74 - ETA: 1:27:10 - loss: 4.0447 - acc: 0.74 - ETA: 1:26:23 - loss: 4.0497 - acc: 0.74 - ETA: 1:25:36 - loss: 4.0529 - acc: 0.74 - ETA: 1:24:49 - loss: 4.0495 - acc: 0.74 - ETA: 1:24:01 - loss: 4.0511 - acc: 0.74 - ETA: 1:23:14 - loss: 4.0527 - acc: 0.74 - ETA: 1:22:27 - loss: 4.0510 - acc: 0.74 - ETA: 1:21:40 - loss: 4.0476 - acc: 0.74 - ETA: 1:20:53 - loss: 4.0508 - acc: 0.74 - ETA: 1:20:06 - loss: 4.0540 - acc: 0.74 - ETA: 1:19:20 - loss: 4.0573 - acc: 0.74 - ETA: 1:18:33 - loss: 4.0555 - acc: 0.74 - ETA: 1:17:46 - loss: 4.0571 - acc: 0.74 - ETA: 1:17:00 - loss: 4.0652 - acc: 0.74 - ETA: 1:16:13 - loss: 4.0635 - acc: 0.74 - ETA: 1:15:27 - loss: 4.0601 - acc: 0.74 - ETA: 1:14:41 - loss: 4.0600 - acc: 0.74 - ETA: 1:13:55 - loss: 4.0583 - acc: 0.74 - ETA: 1:13:08 - loss: 4.0566 - acc: 0.74 - ETA: 1:12:22 - loss: 4.0549 - acc: 0.74 - ETA: 1:11:36 - loss: 4.0565 - acc: 0.74 - ETA: 1:10:50 - loss: 4.0564 - acc: 0.74 - ETA: 1:10:05 - loss: 4.0547 - acc: 0.74 - ETA: 1:09:19 - loss: 4.0530 - acc: 0.74 - ETA: 1:08:33 - loss: 4.0513 - acc: 0.74 - ETA: 1:07:47 - loss: 4.0480 - acc: 0.74 - ETA: 1:07:02 - loss: 4.0464 - acc: 0.74 - ETA: 1:06:16 - loss: 4.0479 - acc: 0.74 - ETA: 1:05:31 - loss: 4.0462 - acc: 0.74 - ETA: 1:04:46 - loss: 4.0446 - acc: 0.74 - ETA: 1:04:00 - loss: 4.0477 - acc: 0.74 - ETA: 1:03:15 - loss: 4.0476 - acc: 0.74 - ETA: 1:02:30 - loss: 4.0492 - acc: 0.74 - ETA: 1:01:44 - loss: 4.0475 - acc: 0.74 - ETA: 1:00:59 - loss: 4.0490 - acc: 0.74 - ETA: 1:00:14 - loss: 4.0458 - acc: 0.74 - ETA: 59:29 - loss: 4.0505 - acc: 0.7462 - ETA: 58:44 - loss: 4.0488 - acc: 0.74 - ETA: 57:59 - loss: 4.0488 - acc: 0.74 - ETA: 57:14 - loss: 4.0456 - acc: 0.74 - ETA: 56:30 - loss: 4.0471 - acc: 0.74 - ETA: 55:45 - loss: 4.0455 - acc: 0.74 - ETA: 55:01 - loss: 4.0407 - acc: 0.74 - ETA: 54:17 - loss: 4.0391 - acc: 0.74 - ETA: 53:33 - loss: 4.0406 - acc: 0.74 - ETA: 52:49 - loss: 4.0437 - acc: 0.74 - ETA: 52:06 - loss: 4.0421 - acc: 0.74 - ETA: 51:23 - loss: 4.0420 - acc: 0.74 - ETA: 50:40 - loss: 4.0466 - acc: 0.74 - ETA: 49:57 - loss: 4.0465 - acc: 0.74 - ETA: 49:14 - loss: 4.0449 - acc: 0.74 - ETA: 48:32 - loss: 4.0433 - acc: 0.74 - ETA: 47:49 - loss: 4.0402 - acc: 0.74 - ETA: 47:06 - loss: 4.0417 - acc: 0.74 - ETA: 46:24 - loss: 4.0416 - acc: 0.74 - ETA: 45:40 - loss: 4.0431 - acc: 0.74 - ETA: 44:57 - loss: 4.0446 - acc: 0.74 - ETA: 44:14 - loss: 4.0430 - acc: 0.74 - ETA: 43:30 - loss: 4.0475 - acc: 0.74 - ETA: 42:47 - loss: 4.0475 - acc: 0.74 - ETA: 42:04 - loss: 4.0459 - acc: 0.74 - ETA: 41:22 - loss: 4.0489 - acc: 0.74 - ETA: 40:39 - loss: 4.0549 - acc: 0.74 - ETA: 39:56 - loss: 4.0533 - acc: 0.74 - ETA: 39:14 - loss: 4.0487 - acc: 0.74 - ETA: 38:30 - loss: 4.0546 - acc: 0.74 - ETA: 37:47 - loss: 4.0531 - acc: 0.74 - ETA: 37:04 - loss: 4.0515 - acc: 0.74 - ETA: 36:21 - loss: 4.0529 - acc: 0.74 - ETA: 35:39 - loss: 4.0529 - acc: 0.74 - ETA: 34:56 - loss: 4.0483 - acc: 0.74 - ETA: 34:13 - loss: 4.0438 - acc: 0.74 - ETA: 33:31 - loss: 4.0467 - acc: 0.74 - ETA: 32:48 - loss: 4.0496 - acc: 0.74 - ETA: 32:06 - loss: 4.0510 - acc: 0.74 - ETA: 31:24 - loss: 4.0480 - acc: 0.74 - ETA: 30:42 - loss: 4.0465 - acc: 0.74 - ETA: 30:00 - loss: 4.0508 - acc: 0.74 - ETA: 29:18 - loss: 4.0493 - acc: 0.74 - ETA: 28:36 - loss: 4.0507 - acc: 0.74 - ETA: 27:54 - loss: 4.0551 - acc: 0.74 - ETA: 27:13 - loss: 4.0535 - acc: 0.74 - ETA: 26:31 - loss: 4.0520 - acc: 0.74 - ETA: 25:49 - loss: 4.0519 - acc: 0.74 - ETA: 25:07 - loss: 4.0504 - acc: 0.74 - ETA: 24:26 - loss: 4.0533 - acc: 0.74 - ETA: 23:44 - loss: 4.0547 - acc: 0.74 - ETA: 23:02 - loss: 4.0531 - acc: 0.74 - ETA: 22:21 - loss: 4.0516 - acc: 0.74 - ETA: 21:40 - loss: 4.0486 - acc: 0.74 - ETA: 20:58 - loss: 4.0442 - acc: 0.74 - ETA: 20:17 - loss: 4.0456 - acc: 0.74 - ETA: 19:35 - loss: 4.0441 - acc: 0.74 - ETA: 18:54 - loss: 4.0484 - acc: 0.74 - ETA: 18:13 - loss: 4.0469 - acc: 0.74 - ETA: 17:31 - loss: 4.0468 - acc: 0.74 - ETA: 16:50 - loss: 4.0482 - acc: 0.74 - ETA: 16:09 - loss: 4.0482 - acc: 0.74 - ETA: 15:28 - loss: 4.0467 - acc: 0.74 - ETA: 14:47 - loss: 4.0452 - acc: 0.74 - ETA: 14:06 - loss: 4.0451 - acc: 0.74 - ETA: 13:25 - loss: 4.0451 - acc: 0.74 - ETA: 12:44 - loss: 4.0436 - acc: 0.74 - ETA: 12:04 - loss: 4.0478 - acc: 0.74 - ETA: 11:23 - loss: 4.0463 - acc: 0.74 - ETA: 10:42 - loss: 4.0505 - acc: 0.74 - ETA: 10:02 - loss: 4.0505 - acc: 0.74 - ETA: 9:21 - loss: 4.0490 - acc: 0.7464 - ETA: 8:41 - loss: 4.0504 - acc: 0.746 - ETA: 8:00 - loss: 4.0531 - acc: 0.746 - ETA: 7:20 - loss: 4.0531 - acc: 0.746 - ETA: 6:40 - loss: 4.0516 - acc: 0.746 - ETA: 5:59 - loss: 4.0473 - acc: 0.746 - ETA: 5:19 - loss: 4.0473 - acc: 0.746 - ETA: 4:39 - loss: 4.0486 - acc: 0.746 - ETA: 3:59 - loss: 4.0471 - acc: 0.746 - ETA: 3:19 - loss: 4.0485 - acc: 0.746 - ETA: 2:39 - loss: 4.0498 - acc: 0.746 - ETA: 1:59 - loss: 4.0498 - acc: 0.746 - ETA: 1:19 - loss: 4.0455 - acc: 0.746 - ETA: 39s - loss: 4.0469 - acc: 0.746 - 31572s 44s/step - loss: 4.0454 - acc: 0.7467 - val_loss: 4.5958 - val_acc: 0.7133\n"
     ]
    }
   ],
   "source": [
    "def vgg19_train(train_dir, val_dir):\n",
    "    \n",
    "    train_gen, val_gen = prepare_training(train_dir=train_dir, test_dir=test_dir)\n",
    "\n",
    "    base_model = VGG19(weights='imagenet', include_top=False)\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "    history = model.fit_generator(train_gen, epochs = NB_EPOCHS, steps_per_epoch = nb_train_samples, \\\n",
    "                                     validation_data=val_gen, validation_steps = nb_val_samples)\n",
    "    model.save(\"vgg19.h5\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "h = vgg19_train(train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "  4/720 [..............................] - ETA: 4:53:50 - loss: 0.8471 - acc: 0.28 - ETA: 3:45:25 - loss: 1.5609 - acc: 0.45 - ETA: 3:21:04 - loss: 1.2564 - acc: 0.59 - ETA: 3:08:15 - loss: 1.1657 - acc: 0.6328"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-12cc68e0db12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-12cc68e0db12>\u001b[0m in \u001b[0;36mresnet_train\u001b[1;34m(train_dir, val_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msetup_to_transfer_learn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNB_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_train_samples\u001b[0m\u001b[1;33m,\u001b[0m                                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"resnet.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def resnet_train(train_dir, val_dir):\n",
    "    \n",
    "    train_gen, val_gen = prepare_training(train_dir=train_dir, test_dir=test_dir)\n",
    "\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "    history = model.fit_generator(train_gen, epochs = NB_EPOCHS, steps_per_epoch = nb_train_samples, \\\n",
    "                                     validation_data=val_gen, validation_steps = nb_val_samples)\n",
    "    model.save(\"resnet.h5\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "h = resnet_train(train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "def predict(test_dir, model):\n",
    "    from keras.models import load_model\n",
    "    test_datagen = ImageDataGenerator()\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    model = load_model(model) # i.e. vgg16.h5\n",
    "    pred = model.predict_generator(test_generator, steps=len(test_generator))\n",
    "    \n",
    "    return pred\n",
    "\n",
    "preds = predict('./val_CAL', 'resnet_2048_32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(direc):\n",
    "    # direc = \"./val_CAL\"\n",
    "    folder = direc + \"/0\"\n",
    "    num0 = len(os.listdir(direc + '/0'))\n",
    "    num1 = len(os.listdir(direc + '/1'))\n",
    "    y_real = list(np.repeat(0, num0)) + list(np.repeat(1, num1))\n",
    "    \n",
    "    return y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(preds):\n",
    "    classes = []\n",
    "\n",
    "    for img in preds:\n",
    "        if img[0] >= img[1]:\n",
    "            classes.append(0)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "            \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8142857142857143\n"
     ]
    }
   ],
   "source": [
    "y_real = get_labels('val_CAL')\n",
    "\n",
    "classes = decode(preds)\n",
    "\n",
    "num_correct = 0\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] == y_real[i]:\n",
    "        num_correct += 1\n",
    "\n",
    "total_acc = num_correct / len(classes)\n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_two(vec):\n",
    "    two_vec = np.zeros((140, 2))\n",
    "    for i in range(len(vec)):\n",
    "        if vec[i] == 0:\n",
    "            two_vec[i][0] = 1\n",
    "        else:\n",
    "            two_vec[i][1] = 1\n",
    "    return two_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcjXX7wPHPNTPMwljG9ojssiRU\nkyWFlCW0UrSoPHokUdEjFaWkTSIitMivekqllKdEiDwVMcqSJYQYyW6sM2a5fn/ct3GMmTNnpjlz\n5sxc79drXs69X/ftnHOd7/d739+vqCrGGGNMVkICHYAxxpiCzRKFMcYYryxRGGOM8coShTHGGK8s\nURhjjPHKEoUxxhivLFGYHBORO0Tkm0DHEWgiUk1EjolIaD4es4aIqIiE5dcx/UlE1olI21xsZ+/B\nfCT2HEVwE5HtQCUgFTgGzAUGqOqxQMZVGLnX+l5VXRDAGGoA24BiqpoSqDjcWBSoq6pb/HycGhSQ\ncy6qrERROFynqiWBpsDFwOMBjidXAvkrubD8Qs8Ju97GV5YoChFV/QuYh5MwABCRcBEZIyI7RGSP\niEwRkUiP5TeIyCoROSIiv4tIJ3d+aRF5W0R2i8guERl1uopFRO4Rke/d11NEZIxnHCLyhYgMdl+f\nJyKfisg+EdkmIg96rPe0iMwUkfdF5AhwT8ZzcuN4193+DxEZLiIhHnH8ICKviUiCiGwUkaszbOvt\nHH4QkXEichB4WkRqi8i3InJARPaLyH9EpIy7/ntANeC/bnXToxmrgURksYg86+73qIh8IyLlPeK5\nyz2HAyLypIhsF5FrMvu/FJFIEXnFXT9BRL73/H8D7nD/T/eLyDCP7ZqJyFIROeye90QRKe6xXEXk\nARHZDGx2540XkZ3ue2CliFzpsX6oiDzhvjeOusvPF5El7iqr3evRw12/q/t+OiwiP4pIY499bReR\noSKyBjguImGe18CNPc6NY4+IjHU3PX2sw+6xWnq+B91tLxSR+SJy0N32icyuq8klVbW/IP4DtgPX\nuK+rAmuB8R7LXwVmAzFANPBf4AV3WTMgAWiP86OhClDfXfY5MBUoAVQElgP3ucvuAb53X7cGdnKm\nGrMscBI4z93nSuApoDhQC9gKdHTXfRpIBm50143M5PzeBb5wY68BbAL6eMSRAgwCigE93POJ8fEc\nUoCBQBgQCdRxr0U4UAHnC+rVzK61O10DUCDMnV4M/A5c4O5vMfCiu6whTtXgFe61GOOe+zVZ/L9O\ncrevAoQCl7txnT7mm+4xmgBJQAN3u0uBFu451QA2AA977FeB+Tjvh0h33p1AOXebR4C/gAh32RCc\n91Q9QNzjlfPYVx2PfV8C7AWauzHf7V6zcI/rtwo43+PY6dcUWAr0cl+XBFpkdp0zeQ9GA7vd2CPc\n6eaB/mwWpr+AB2B/f/M/0PmgHQOOuh+mhUAZd5kAx4HaHuu3BLa5r6cC4zLZZyX3yyfSY95twCL3\nteeHVIAdQGt3+l/At+7r5sCODPt+HHjHff00sMTLuYW6cTT0mHcfsNgjjj9xk5Q7bznQy8dz2JHV\nsd11bgR+yXCts0sUwz2W9wfmuq+fAj70WBYFnCKTRIGTNE8CTTJZdvqYVTOcc88szuFhYJbHtALt\nsjnvQ6ePDfwG3JDFehkTxWTg2Qzr/Aa08bh+/8zk/Xs6USwBngHKZ3HOWSWK2zz/n+wv7/+snrBw\nuFFVF4hIG+ADoDxwGOdXcRSwUkROrys4X8Dg/LKbk8n+quP8Qt/tsV0ITsnhLKqqIjID58O6BLgd\neN9jP+eJyGGPTUKB/3lMn7NPD+Vxfn3/4THvD5xf2aftUvfbwmP5eT6ew1nHFpGKwATgSpxfpSE4\nX5o58ZfH6xM4v4xxY0o/nqqeEJEDWeyjPM4v499zehwRuQAYC8Ti/N+H4ZTqPGU870eAe90YFSjl\nxgDOe8RbHJ6qA3eLyECPecXd/WZ67Az6ACOBjSKyDXhGVb/04bg5idHkgrVRFCKq+h0wHadaA2A/\nzi/TC1W1jPtXWp2Gb3A+tLUz2dVOnF/j5T22K6WqF2Zx6A+B7iJSHacU8anHfrZ57KOMqkaramfP\nsL2c0n6c6pnqHvOqAbs8pquIRyZwl//p4zlkPPYL7rzGqloKp0pGvKyfE7txqgYBpw0Cp7onM/uB\nRDL/v8nOZGAjzt1IpYAnOPscwOM83PaIocCtQFlVLYNTfXd6m6zeI5nZCTyX4f87SlU/zOzYGanq\nZlW9Daea8CVgpoiU8LZNLmI0uWCJovB5FWgvIk1VNQ2nLnuc+2sZEakiIh3ddd8GeovI1SIS4i6r\nr6q7gW+AV0SklLustltiOYeq/gLsA94C5qnq6RLEcuCI24AZ6TaMNhKRy3w5EVVNBT4GnhORaDcR\nDeZMiQWcL5UHRaSYiNwCNADm5PQcXNE41XiHRaQKTv28pz047Sy5MRO4TkQudxuXn+HcL3AA3P+3\nacBYcW4GCHUbcMN9OE40cAQ4JiL1gft9WD8F5/8vTESewilRnPYW8KyI1BVHYxE5neAyXo83gX4i\n0txdt4SIdBGRaB/iRkTuFJEK7vmffg+lurGlkfW1/xL4h4g8LM7NG9Ei0tyXYxrfWKIoZFR1H04D\n8JPurKHAFmCZOHcWLcBpmERVlwO9gXE4vyK/48yv97twqg3W41S/zAQqezn0h8A1OFVfp2NJBa7D\nuQtrG84v5beA0jk4pYE47Sxbge/d/U/zWP4TUNfd93NAd1U9XaWT03N4BqdBNgH4Cvgsw/IXgOHu\nHT3/zsE5oKrr3HOZgVO6OIrT8JuUxSb/xmlEXgEcxPmF7cvn9d841X9Hcb64P8pm/XnA1zg3CfyB\nU5LxrB4ai5Osv8FJQG/jNKKD08b0f+71uFVV43DaqCbiXO8tZHInmxedgHUicgwYj9PukqiqJ3D+\nb39wj9XCcyNVPYpzE8J1OFVym4GrcnBckw174M4ELRG5B+cBuCsCHUtOiUhJnF/NdVV1W6DjMcYb\nK1EYk09E5DoRiXLr3cfglBi2BzYqY7JnicKY/HMDTkP7nzjVZT3VivQmCFjVkzHGGK+sRGGMMcar\noHvgrnz58lqjRo1Ah2GMMUFl5cqV+1W1Qm62DbpEUaNGDeLi4gIdhjHGBBUR+SP7tTJnVU/GGGO8\nskRhjDHGK0sUxhhjvLJEYYwxxitLFMYYY7yyRGGMMcYrvyUKEZkmIntF5NcslouITBCRLSKyRkQu\n8Vcsxhhjcs+fJYrpON0GZ+VanP5u6gJ9cQZcMcYYk8dOnUr9W9v77YE7VV0iIjW8rHID8K7bKdoy\nESkjIpXdAWeMMSZvfNYFtmU24m/RMOS/7fnlT2/DsGQvkE9mV+HsAVLi3XnnJAoR6YtT6qBatWr5\nEpwxJg8U8S/pgqDRP/Yy4fu/N+BfIBNFZsNAZtqVraq+AbwBEBsba93dGhMsCkqSqNkZbv4q0FHk\ni/Xr9/Hzz7u5887GANylSpsXE6hZc1Su9xnIRBEPnO8xXRWnn35jTEGV2xLCI/b7zt9OnEhm1Kgl\nvPzyj4SGCi1aVKVOnRhEhBo1yvytfQcyUcwGBojIDKA5kGDtE8a4ClOVTc3OgY6g0Pv668088MAc\ntm07DECfPpdSrlxkNlv5zm+JQkQ+BNoC5UUkHhgBFANQ1SnAHKAzzgDsJ4De/orFmKBTkJNEEarG\nKeh27TrCww/PY+bM9QA0blyJKVO60LLl+dlsmTP+vOvptmyWK/CAv45vTNDJrBRhVTbGiwcemMMX\nX/xGVFQxRo5sy0MPtSAsLO+fegi68SiMKXSyqmayKhuTiZSUtPRk8NJL11CsWCivvNKBatVK++2Y\nliiMCTTPJGHVOiYLCQmJDB/+LZs2HWTu3DsQEerVK88nn9zi92NbojAmEKyayfhIVfnkk/U8/PBc\ndu8+RmiosGrVX1x88d97iC4nLFEYk5+smsnkwO+/H2TAgK+ZO3cLAC1bVmXKlK40blwpX+OwRGFM\nfrJqJuOjMWN+5MknF5GYmEKZMhG89NI13HvvJYSEZPassn9ZojDGX7w9C2HVTCYbJ04kk5iYQq9e\njRkzpgMVK5YIWCyWKIzJCzl5QM6qmUwm9u07zm+/HeCKK5z+7IYObUXbtjVo3bp6gCOzRGFM3sgq\nSVj1kslGWpoybdovPProfMLCQti4cQAxMZGEh4cViCQBliiM8Z0vpQarUjI58Ouve+nX70t++MHp\nSLt9+1qcOJFMTEzedb+RFyxRGAN507eSVSkZHx0/foqRI79j7NhlpKSkUalSCV59tRM9elyISP43\nVmfHEoUp3PK6cz2rSjJ5oHv3T5g7dwsi0L9/LM89dzVlykQEOqwsWaIwhVtOkoQlAZNPhg5txZ49\nx5g8uQvNm1cNdDjZskRhCqeMJQlrOzABkpKSxmuv/cT27YcZP/5aANq2rUFcXN+APBORG5YoTHDJ\nTVWStR2YAFm+fBf33fclq1b9BUDfvpdy4YUVAYImSYAlChNsrCrJBIHDhxN54omFTJkShypUr16a\niRM7pyeJYGOJwgQHq0oyQWLGjF95+OG57NlznLCwEB55pCVPPtmaEiWKBzq0XLNEYQou60DPBKFv\nvvmdPXuO06rV+Uye3IWLLsrfDvz8wRKFKbgyJgmrSjIFUFJSCrt2HaVWrbIAjB7dniuvrMbddzcN\nqnYIbyxRmMCyp51NEPv2223cf/9XhIQIq1f3o3jxUMqXj6J374sDHVqeyvvBVY3JieyShFUzmQJo\nz55j9Oo1i6uvfpdNmw4AEB9/JMBR+Y+VKEzBYKUGEwTS0pQ331zJY48t5PDhRCIiwhg+/EqGDGlF\n8eKhgQ7PbyxRGP/L6240jAmQm276iNmzfwOgY8faTJrUmdq1YwIclf9ZojD+42uCsOolEyRuvrk+\ny5fvYvz4TtxyS8MC2YGfP1iiMP5jw36aIDd79m/Exx+hf//LALjrribcfHMDoqPDAxxZ/rJEYXyX\n2yoka38wQWbHjgQefPBrvvjiN8LDQ+nUqQ61apVFRIpckgBLFAb824Zg1UomiCQnpzJhwk+MGLGY\n48eTiY4uzqhR7ahevXSgQwsoSxTG+k8yBli2LJ777vuSNWv2AHDLLQ0ZN64jVaqUCnBkgWeJorDL\nSWnBqohMEfbkk4tYs2YPNWuWYeLEznTuXDfQIRUYligKI+uK25hsqSpHj56iVCmnzWHixGt5993V\nDBvWmqioYgGOrmCxRFEYWR9Jxnj122/76d9/DiIwf34vRIR69crz3HNXBzq0AskSRbDzVnqwqiRj\nzpKYmMILL/yPF1/8gVOnUilXLpLt2w9Ts2bZQIdWoFmiCDb2EJsxuTJ//u/07z+HLVsOAvDPfzZl\n9Oj2lCsXFeDICj6/JgoR6QSMB0KBt1T1xQzLqwH/B5Rx13lMVa2vB2+yGp/BqpaMyZSq0qfPbN55\nZxUADRtWYMqULlx5ZfUARxY8/JYoRCQUmAS0B+KBFSIyW1XXe6w2HPhYVSeLSENgDlDDXzEFHatW\nMuZvExFq1ChDZGQYTz3VhsGDWxbqDvz8wZ8limbAFlXdCiAiM4AbAM9EocDpm5RLA3/6MZ7gk1WS\nsGolY7xateovdu8+yrXXOre4Dh3ail69GltbRC75M1FUAXZ6TMcDzTOs8zTwjYgMBEoA12S2IxHp\nC/QFqFatWp4HWuBZ6cEYnxw9msSIEYsZP/4nypWLZOPGAcTERBIeHmZJ4m/wZ6LIrFvFjN94twHT\nVfUVEWkJvCcijVQ17ayNVN8A3gCIjY0t3N+a1iW3MTmmqnz++UYefHAu8fFHCAkRbr/9IooVs7HZ\n8oI/E0U8cL7HdFXOrVrqA3QCUNWlIhIBlAf2+jGugi2zZyCMMVn644/DDBjwNV9+uQmA2NjzmDq1\nK5dcUjnAkRUe/kwUK4C6IlIT2AX0BG7PsM4O4Gpguog0ACKAfX6MKXhYdZMx2VJVunX7mJUrd1Oq\nVDjPP9+Ofv1iCQ21kkRe8luiUNUUERkAzMO59XWaqq4TkZFAnKrOBh4B3hSRQTjVUveoqn1DGmO8\nSktTQkIEEWHMmA5MmRLHuHEdqVw5OtChFUoSbN/LsbGxGhcXF+gw/OcVt2nHShTGnOPAgRM89tgC\nAN588/oARxNcRGSlqsbmZlsrnxljCjxV5f/+bxX160/irbd+4d131xAffyTQYRUZ1oVHoNldTsZ4\ntWHDPu6//yu+++4PANq2rcHkyV2oWtXGicgvligCLasuOYwp4lSVp55axEsv/UBychrly0fxyisd\n6NWrMSKZ3X1v/MUSRUFhbRLGnEVE2LXrKMnJafzrX5fw4ovXEBMTGeiwiiRLFMaYAuPPP4+yf/8J\nGjeuBMDo0e3p0+diWrUqgj0yFCDWmG2MCbjU1DQmTlxOgwaT6NlzJqdOpQJQvnyUJYkCwEoUxpiA\n+vnn3dx335fExTkdN7RuXZ0jR5IoX97GiSgofEoUIlIcqKaqW/wcjzGmiDhyJIknn/yWiRNXkJam\nVK1aigkTOnHjjfWtsbqAyTZRiEgXYCxQHKgpIk2BEap6k7+DM8YUTqpK69bvsHr1HkJDhcGDW/D0\n022Jjg4PdGgmE760UYzE6R78MICqrgLq+DMoY0zhJiIMGtSCZs2qEBfXl1de6WhJogDzpeopWVUP\nZygK2r2cxhifnTqVytixSwkNFYYMaQXAXXc14c47G1sHfkHAl0SxQURuBULcnmAfApb5NyxjTGHx\nv//9Qb9+X7F+/T7Cw0O5664mVKpUEhEhNNTaIoKBL6l8AHApkAZ8BiTiJAtjjMnS/v0n+Oc/v6B1\n6+msX7+PunVj+PLL26lUqWSgQzM55EuJoqOqDgWGnp4hIjfjJA1jjDmLqjJ9+iqGDJnPgQMnKV48\nlMcfv4LHHruCiAi7Iz8Y+VKiGJ7JvGF5HYgxpvB4//21HDhwknbtarJmTT+efrqtJYkgluX/nIh0\nxBmmtIqIjPVYVAqnGsrklvUYawqZEyeSSUhIpHLlaESE11/vzIoVf3LHHRfZMxGFgLcUvxf4FadN\nYp3H/KPAY/4MqtCzcbFNIfL115t54IE51KpVlvnzeyEi1KtXnnr1ygc6NJNHskwUqvoL8IuI/EdV\nE/MxpqLDeow1QWzXriM8/PA8Zs5cD0B0dDgHDpy0rjcKIV8qDauIyHNAQyDi9ExVvcBvURljCqzU\n1DQmTVrB8OHfcvToKUqUKMbIkVfx4IPNCQuzZyIKI18SxXRgFDAGuBbojbVRGFMkpaUpbdpM54cf\ndgJw4431GT++E9WqlQ5wZMaffEn/Uao6D0BVf1fV4cBV/g3LGFMQhYQIHTrU5vzzS/HFFz2ZNauH\nJYkiwJcSRZI4ty38LiL9gF1ARf+GVQjYnU2mEFBVPv54HWFhIXTr1hCAoUNbMXhwS0qWLB7g6Ex+\n8SVRDAJKAg8CzwGlgX/6M6hCIbskYXc6mQLu998P0r//HL755ncqVIiiXbualC0bSXh4GOHWf1+R\nkm2iUNWf3JdHgV4AIlLVn0EVKnZnkwkySUkpvPzyjzz33P9ITEyhbNkInnuuHaVLR2S/sSmUvCYK\nEbkMqAJ8r6r7ReRCnK482gGWLDJjVU4miC1evJ377/+KjRv3A9CrV2PGjOlAxYolAhyZCaQsG7NF\n5AXgP8AdwFwRGQYsAlYDdmtsVjyThFUvmSCSmppG//5OkqhXrxzffnsX7757kyUJ47VEcQPQRFVP\nikgM8Kc7/Vv+hBbkrMrJBIG0NCUxMYWoqGKEhoYweXIXliz5g0cfbUV4uPXNZBze3gmJqnoSQFUP\nishGSxLGFB5r1+6hX7+vqF+/HG+/fQMAbdrUoE2bGoENzBQ43hJFLRE53ZW4ADU8plHVm/0amTHG\nL44fP8XIkd8xduwyUlLS2LbtEIcOnaRs2chAh2YKKG+JoluG6Yn+DMQY43///e9vDBjwNTt2JCAC\n/fvH8txzV1OmjN3RZLLmrVPAhfkZSFCyO5xMkEhJSaNHj5l89tkGAJo2/QdTp3alWbMqAY7MBANr\nrfo7skoSdreTKWDCwkIoXTqckiWL8+yzVzFgQDPrwM/4TFT9d3eOiHQCxgOhwFuq+mIm69wKPA0o\nsFpVb/e2z9jYWI2Li/NDtDmQsSRhdziZAuinn+IBaN7ceeTpwIETnDyZQtWqpQIZlgkQEVmpqrG5\n2dbnEoWIhKtqUg7WDwUmAe2BeGCFiMxW1fUe69QFHgdaqeohEQmOPqTsWQlTgB0+nMjjjy9g6tSV\n1K9fnlWr+lG8eCjlytk4ESZ3sk0UItIMeBunj6dqItIEuFdVB2azaTNgi6pudfczA+fZjPUe6/wL\nmKSqhwBUdW/OTyGArCRhChBV5cMPf2Xw4Hns2XOcsLAQrr++HqmpaTiFemNyx5cSxQSgK/A5gKqu\nFhFfuhmvAuz0mI4HmmdY5wIAEfkB5538tKrO9WHf+c8ark0BtnnzAfr3n8OCBVsBaNXqfKZM6Uqj\nRsFRSDcFmy+JIkRV/8gwQHqqD9tlNqJ6xp/gYUBdoC1O31H/E5FGqnr4rB2J9AX6AlSrVs2HQ/uB\njXNtCqjk5FTatXuX+PgjxMREMnr0NfTufTEhIZl9BI3JOV8SxU63+knddoeBwCYftosHzveYrorT\nDUjGdZapajKwTUR+w0kcKzxXUtU3gDfAacz24dj+Y9VNpoBQVUSEYsVCee65dixatJ3Ro6+hQgXr\nm8nkLV/uj7sfGAxUA/YALdx52VkB1BWRmiJSHOgJzM6wzue4o+WJSHmcqqitvoVuTNG0Z88xevWa\nxahRS9Ln3XVXE9555wZLEsYvfClRpKhqz5zuWFVTRGQAMA+n/WGaqq4TkZFAnKrOdpd1EJH1ONVZ\nQ1T1QE6PZUxRkJamvPnmSh57bCGHDydSpkwEDz/cguhoG0XI+JcviWKFWyX0EfCZqh71deeqOgeY\nk2HeUx6vFae0MtjXfRpTFK1e/Rf9+n3FsmXOsxGdOtVh0qTOliRMvvBlhLvaInI5TtXRMyKyCpih\nqjP8Hp0xRVxyciqPP76QV19dRmqqUrlyScaP70T37g3JcIOJMX7j0zP8qvqjqj4IXAIcwRnQyBjj\nZ2FhIfzyy1+kpSkDBzZjw4YHuOWWCy1JmHzlywN3JXEelOsJNAC+AC73c1wFhz0/YfLZjh0JpKam\nUbNmWUSEKVO6kJCQRGzseYEOzRRRvrRR/Ar8Fxitqv/zczwFj3XXYfJJcnIq48f/xIgRi2nZsirz\n5/dCRKhbt1ygQzNFnC+Jopaqpvk9koLOnp8wfrR06U769fuKNWv2ABATE8mJE8mUKFE8wJEZ4yVR\niMgrqvoI8KmInPMtWahHuLPqJpNPDh06yWOPLeCNN34GoGbNMkya1Jlrr60b4MiMOcNbieIj99+i\nN7Kddddh8kFSUgpNm05lx44EihULYciQyxk2rDVRUcUCHZoxZ/E2wt1y92UDVT0rWbgP0hX+EfCs\nusn4UXh4GH36XMzChduYPLkLDRtWCHRIxmTKl9tj/5nJvD55HYgxhV1iYgojRizigw/Wps974okr\nWbz4bksSpkDz1kbRA+eW2Joi8pnHomjgcOZbGWMyM3/+7/TvP4ctWw5SsWIJbrqpPpGRxWw4UhMU\nvLVRLAcO4PT6Oslj/lHgF38GZUxh8ddfxxg8eB4ffvgrABdeWIEpU7oSGWntECZ4eGuj2AZsAxbk\nXzgBZnc7mTySmprG1KkreeKJhSQkJBEZGcaIEW0YNKglxYvbaHMmuHirevpOVduIyCHOHnBIcPrz\ni/F7dPnNHq4zeSQ1VXntteUkJCTRuXNdJk68lpo1ywY6LGNyxVvV0+nhTsvnRyAFit3tZHLh6NEk\nUlOVMmUiKF48lDffvI49e45x880NrG8mE9SybEnzeBr7fCBUVVOBlsB9gI2OYoxLVfnssw00aDCJ\nRx6Zlz7/iiuq0a2b9fJqgp8vt1x8jjMMam3gXZyOAT/wa1TGBInt2w9z/fUz6NbtY3btOsqvv+4j\nMTEl0GEZk6d8SRRp7pjWNwOvqupAoIp/wzKmYEtOTuWll76nYcNJfPnlJkqVCmfixGv58cd/EhHh\nSxdqxgQPn4ZCFZFbgF7Aje68wnVvn93tZHLgxIlkWrR4i7Vr9wLQs2cjxo7tQOXK0QGOzBj/8CVR\n/BPoj9PN+FYRqQl86N+w8pnd7WRyICqqGLGx53HiRDKvv96FDh1qBzokY/xKnGGrs1lJJAyo405u\nUdWAVcLGxsZqXFxc7nfgrfRgdzuZTKgq7767mtq1Y7jiimoAJCQkUrx4qD04Z4KGiKxU1djcbOvL\nCHdXAu8Bu3CeofiHiPRS1R9yc8CAyypJWEnCZGLDhn3cf/9XfPfdHzRoUJ5Vq/pRvHgopUtHBDo0\nY/KNL1VP44DOqroeQEQa4CSOXGWmAsNKD8aLkyeTee65/zF69A8kJ6dRoUIUjz9+BcWKWd9Mpujx\nJVEUP50kAFR1g4gEx7Bb1khtcmHu3C088MActm49BMC//nUJL754DTExkQGOzJjA8CVR/CwiU3FK\nEQB3UBA7BcxJUrBqJpOFY8dO0avXLPbvP0GjRhWZMqULrVpVC3RYxgSUL4miH/Ag8ChOG8US4DV/\nBpUr3toebv4qf2MxQSU1NY20NKVYsVBKlizO+PGdiI8/wqBBLShWzDrwM8ZrohCRi4DawCxVHZ0/\nIf1N1vZgcmDlyj+5774vueGGejz5ZBsAbr/9ogBHZUzBkmXLnIg8gdN9xx3AfBHJbKQ7Y4LSkSNJ\nPPTQ1zRr9hYrV+7mvffWkJycGuiwjCmQvJUo7gAaq+pxEakAzAGm5U9YxviHqjJz5noeemguu3cf\nIzRUGDy4Bc88c5VVMxmTBW+JIklVjwOo6j4RsfsCTVA7ejSJHj1m8vXXWwBo3rwKU6Z0pWnTfwQ4\nMmMKNm+JopbHWNkC1PYcO1tVb/ZrZMbksZIli5OUlErp0uG8+OI19O17KSEh1gW4Mdnxlii6ZZie\n6M9AjPGHJUv+oHLlktStWw4RYdq064mICKNSpZKBDs2YoOFtzOyF+RmIMXlp//4TPProfN55ZxVX\nX12T+fN7ISJUr14m0KEZE3TJnIQEAAAd5klEQVSs43xTqKSlKdOnr2LIkPkcPHiS4sVDufLKaqSm\nKmFhVs1kTG74tYFaRDqJyG8iskVEHvOyXncRUREJ7v6jTECtW7eXtm2n06fPbA4ePMnVV9dk7dr7\nGTGiLWFhdi+GMbnlc4lCRMJVNSkH64cCk4D2QDywQkRme/Yb5a4XjfPk90++7tuYjBISEmnR4m2O\nHTtFxYolGDu2A7fffpGNV21MHsj2Z5aINBORtcBmd7qJiPjShUcznLErtqrqKWAGcEMm6z0LjAYS\nfQ/bGMfp8VRKl45g6NBW9Ot3KRs3PsAddzS2JGFMHvGlPD4B6AocAFDV1cBVPmxXBdjpMR1PhrG2\nReRi4HxV/dLbjkSkr4jEiUjcvn37fDi0Kex27TpC9+4f8/77a9LnDRt2JZMnd6VsWevl1Zi85Eui\nCFHVPzLM86Wvg8x+zqV3xOQ+wDcOeCS7HanqG6oaq6qxFSpU8OHQprBKSUlj/Phl1K8/iU8/3cCI\nEYtJTU0DsBKEMX7iSxvFThFpBqjb7jAQ2OTDdvHA+R7TVYE/PaajgUbAYvcD/g9gtohcr6p/Y6xT\nU1itWLGLfv2+4uefdwNw4431mTChE6Gh1lBtjD/5kijux6l+qgbsARa487KzAqgrIjVxhlHtCdx+\neqGqJgDlT0+LyGLg35YkTEbHj59i6NAFvP76ClShWrXSvPbatVx/fb1Ah2ZMkZBtolDVvThf8jmi\nqikiMgCYB4QC01R1nYiMBOJUdXaOozVFUlhYCAsWbCUkRBg8uCUjRrShRIngGGTRmMIg20QhIm/i\n0bZwmqr2zW5bVZ2D0+us57ynsli3bXb7M0XH778fpEyZCMqViyI8PIz33ruJiIgwLrqoUqBDM6bI\n8aVydwGw0P37AagI+Pw8hTE5kZSUwqhRS2jUaDJDhy5In3/ZZVUsSRgTIL5UPX3kOS0i7wHz/RaR\nKbIWL97O/fd/xcaN+wHnDqfU1DRrrDYmwHLT11NNoHpeB2KKrr17jzNkyHzefXc1APXqlWPy5C5c\ndVXNAEdmjAHf2igOcaaNIgQ4CGTZb5MxObF//wkaNJjEwYMnCQ8PZdiwK3n00VaEh1t/lcYUFF4/\njeI84NAE5/ZWgDQ93WeCMXmgfPkobrihHvHxR3j99S7UqRMT6JCMMRl4TRSqqiIyS1Uvza+ATOF2\n/PgpRo78ji5dLqB1a6cG8/XXuxAeHmpPVhtTQPnSSrhcRC7xeySm0Pvvf3+jYcPXGT36R/r3/4q0\nNKdwGhERZknCmAIsyxKFiISpagpwBfAvEfkdOI7Th5OqqiUP45OdOxN46KG5zJq1EYCLL/4HU6d2\ntfGqjQkS3qqelgOXADfmUyy581kX2DYn+/VMvktJSWPChJ946qlFHD+eTMmSxRk16ioeeKCZDSRk\nTBDxligEQFV/z6dYcsczSdTsHLg4zDmOHEnihRe+5/jxZLp1a8Crr3aiatVSgQ7LGJND3hJFBREZ\nnNVCVR3rh3hy7xG7GasgOHw4kcjIMMLDw4iJiWTq1K6Eh4fSpcsFgQ7NGJNL3sr/oUBJnO7AM/sz\nJp2q8sEHa6lXbyKjR/+QPv/mmxtYkjAmyHkrUexW1ZH5FokJWps2HaB//69YuHAbAEuW7EBV7U4m\nYwqJbNsojMlKYmIKL730Pc8//z2nTqUSExPJyy+35557mlqSMKYQ8ZYors63KHLK7nQKuL/+Okbr\n1u+wefNBAO65pykvv9ye8uWjAhyZMSavZZkoVPVgfgaSIxmThN3tlO8qVSrB+eeXJiwshMmTu9Cm\nTY1Ah2SM8ZPg7nnN7nTKN2lpyptvruSqq2pywQXlEBE++OBmypaNpHjx0ECHZ4zxI3vqyWRr9eq/\naNVqGv36fUX//l9xul/ISpVKWpIwpggI7hKF8atjx07x9NOLefXVZaSmKuedF02/frGBDssYk88s\nUZhMff75RgYO/Jr4+COEhAgDBzZj1Kh2lCoVHujQjDH5zBKFOceuXUfo2XMmSUmpXHppZaZM6Ups\n7HmBDssYEyCWKAwAycmphIWFICJUqVKK555rR/HiofTvf5mNWW1MEWffAIYff9zJpZe+wfvvr0mf\n98gjlzNwYHNLEsYYSxRF2cGDJ7nvvv/SqtU01q7dy+uvx2Ej3RpjMrKqpyJIVXn//TU88sg37Nt3\ngmLFQnj00VYMG3aldb1hjDmHJYoiZs+eY9x226csWrQdgDZtqjN5chcaNKgQ2MCMMQWWJYoipkyZ\nCHbvPkb58lGMGdOeu+5qYqUIY4xXliiKgPnzf+eSSypTrlwU4eFhfPLJLVSuXJJy5awDP2NM9qwx\nuxDbvfsot932KR06vM/QoQvS5zdqVNGShDHGZ1aiKIRSU9OYOnUljz++kCNHkoiMDKNevXI2mJAx\nJlcsURQyP/+8m379vmTFij8B6NKlLhMndqZGjTIBjswYE6wsURQi27cfplmzN0lNVapUiWbChGu5\n6ab6Voowxvwtfk0UItIJGA+EAm+p6osZlg8G7gVSgH3AP1X1D3/GVJjVqFGG3r2bEh0dzjPPtCU6\n2jrwM8b8fX5rzBaRUGAScC3QELhNRBpmWO0XIFZVGwMzgdH+iqcw2r79MNdd9yHffbc9fd4bb1zH\n2LEdLUkYY/KMP0sUzYAtqroVQERmADcA60+voKqLPNZfBtzpx3gKjeTkVMaOXcozz3zHyZMp7N9/\ngqVL+wBYNZMxJs/5M1FUAXZ6TMcDzb2s3wf4OrMFItIX6AtQrVq1vIovKH3//Q769fuSdev2AdCz\nZyPGju0Q4KiMMYWZPxNFZj9tM+1xTkTuBGKBNpktV9U3gDcAYmNjFXbkVYxB49ChkwwZMp+33/4F\ngNq1y/L6613o0KF2gCMzxhR2/kwU8cD5HtNVgT8zriQi1wDDgDaqmuTHeIJaWpryxRe/UaxYCI89\ndgWPP34FkZHFAh2WMaYI8GeiWAHUFZGawC6gJ3C75woicjEwFeikqnv9GEtQ2rhxPzVrliE8PIxy\n5aL4z39uplq10tSvXz7QoRljihC/3fWkqinAAGAesAH4WFXXichIEbneXe1loCTwiYisEpHZ2e54\nz0p/hVxgnDiRzLBhC2nceDKjR/+QPr9Dh9qWJIwx+c6vz1Go6hxgToZ5T3m8vibXO6/ZOfeBFWBz\n526hf/+v2LbtMAD7958IcETGmKIuOJ/MfqTwjcL2559HefjhuXzyiXP38EUXVWTKlK5cfvn52Wxp\njDH+FZyJopDZtOkAsbFvcPToKaKiivH00214+OEWFCsWGujQjDHGEkVBULduDJddVoUSJYrx2mvX\nUr26deBnjCk4LFEEwJEjSTz11CL697+MCy4oh4gwe3ZPSpQoHujQjDHmHJYo8pGqMnPmeh56aC67\ndx9j48b9zJ3r9FpiScIYU1BZosgnW7ceYsCAOXz99RYAWrSoyksv5f6mL2OMyS+WKPzs1KlUxoz5\nkWefXUJiYgplykTw4otX869/XUpIiHXgZ4wp+CxR+NnOnQmMHPkdSUmp3HHHRbzySgcqVSoZ6LCM\nMcZnlij84NChk5QpE4GIULt2DOPHd6JOnRiuvrpWoEMzxpgc81sXHkVRWpoybdov1KnzGu+/vyZ9\n/n33xVqSMMYELUsUeWTdur20bTudPn1mc/DgyfRGa2OMCXZW9fQ3nTiRzLPPfseYMUtJSUmjYsUS\njBvXkdtuaxTo0IwxJk9YovgbNm06QMeO77N9+2FEoF+/S3n++aspWzYy0KEZY0yesUTxN1SvXpqI\niDCaNKnElCldadGiaqBDMgVIcnIy8fHxJCYmBjoUU4RERERQtWpVihXLu4HNLFHkQEpKGlOmxHHb\nbY0oVy6K8PAw5s69gypVShEWZs095mzx8fFER0dTo0YNROyZGeN/qsqBAweIj4+nZs2aebZf+3bz\n0fLlu2jW7E0GDvyaoUMXpM+vXr2MJQmTqcTERMqVK2dJwuQbEaFcuXJ5Xoq1EkU2EhISGTbsW15/\nfQWqUK1aaW64oV6gwzJBwpKEyW/+eM9ZosiCqvLRR+sYNGgef/11jLCwEAYPbsFTT7WxDvyMMUWK\n1ZlkYfXqPdx226f89dcxLr/8fH7+uS8vvdTekoQJKqGhoTRt2pRGjRpx3XXXcfjw4fRl69ato127\ndlxwwQXUrVuXZ599FtUzo0d+/fXXxMbG0qBBA+rXr8+///3vQJyCV7/88gv33ntvoMPw6oUXXqBO\nnTrUq1ePefPmZbrOwoULueSSS2jatClXXHEFW7Y4z2ENGjSIpk2b0rRpUy644ALKlHHGqtm3bx+d\nOnXKt3NAVYPq79KqqL+kpKSeNT1o0Fx9882Vmpqa5rdjmsJr/fr1gQ5BS5Qokf76rrvu0lGjRqmq\n6okTJ7RWrVo6b948VVU9fvy4durUSSdOnKiqqmvXrtVatWrphg0bVFU1OTlZJ02alKexJScn/+19\ndO/eXVetWpWvx8yJdevWaePGjTUxMVG3bt2qtWrV0pSUlHPWq1u3bvr7ZdKkSXr33Xefs86ECRO0\nd+/e6dP33HOPfv/995keN7P3HhCnufzetaon16JF2+jffw5Tp3aldevqAIwd2zHAUZlC4xU/tVXk\nYPz4li1bsmaN07XMBx98QKtWrejQoQMAUVFRTJw4kbZt2/LAAw8wevRohg0bRv369QEICwujf//+\n5+zz2LFjDBw4kLi4OESEESNG0K1bN0qWLMmxY8cAmDlzJl9++SXTp0/nnnvuISYmhl9++YWmTZsy\na9YsVq1alf5LuU6dOvzwww+EhITQr18/duzYAcCrr75Kq1atzjr20aNHWbNmDU2aNAFg+fLlPPzw\nw5w8eZLIyEjeeecd6tWrx/Tp0/nqq69ITEzk+PHjfPvtt7z88st8/PHHJCUlcdNNN/HMM88AcOON\nN7Jz504SExN56KGH6Nu3r8/XNzNffPEFPXv2JDw8nJo1a1KnTh2WL19Oy5Ytz1pPRDhy5AgACQkJ\nnHfeeefs68MPP0yP83Ss//nPf865Lv5Q5BPF3r3HGTJkPu++uxqAsWOXpicKYwqL1NRUFi5cSJ8+\nfQCn2unSSy89a53atWtz7Ngxjhw5wq+//sojjzyS7X6fffZZSpcuzdq1awE4dOhQttts2rSJBQsW\nEBoaSlpaGrNmzaJ379789NNP1KhRg0qVKnH77bczaNAgrrjiCnbs2EHHjh3ZsGHDWfuJi4ujUaMz\nPSDUr1+fJUuWEBYWxoIFC3jiiSf49NNPAVi6dClr1qwhJiaGb775hs2bN7N8+XJUleuvv54lS5bQ\nunVrpk2bRkxMDCdPnuSyyy6jW7dulCtX7qzjDho0iEWLFp1zXj179uSxxx47a96uXbto0aJF+nTV\nqlXZtWvXOdu+9dZbdO7cmcjISEqVKsWyZcvOWv7HH3+wbds22rVrlz4vNjaW4cOHZ3e580SRTRRp\nacrbb//M0KELOHQokfDwUIYPb82QIZcHOjRTGOXgl39eOnnyJE2bNmX79u1ceumltG/fHnCqnLO6\nOyYnd80sWLCAGTNmpE+XLVs2221uueUWQkNDAejRowcjR46kd+/ezJgxgx49eqTvd/369enbHDly\nhKNHjxIdHZ0+b/fu3VSoUCF9OiEhgbvvvpvNmzcjIiQnJ6cva9++PTExMQB88803fPPNN1x88cWA\nUyravHkzrVu3ZsKECcyaNQuAnTt3snnz5nMSxbhx43y7OHBWm89pmV3fcePGMWfOHJo3b87LL7/M\n4MGDeeutt9KXz5gxg+7du6dfN4CKFSvy559/+hzL31EkE8W2bYe4885Z/PjjTgA6dKjNpEmdqVMn\nJsCRGZO3IiMjWbVqFQkJCXTt2pVJkybx4IMPcuGFF7JkyZKz1t26dSslS5YkOjqaCy+8kJUrV6ZX\n62Qlq4TjOS/jPf0lSpRIf92yZUu2bNnCvn37+Pzzz9N/IaelpbF06VIiI7PuDicyMvKsfT/55JNc\nddVVzJo1i+3bt9O2bdtMj6mqPP7449x3331n7W/x4sUsWLCApUuXEhUVRdu2bTN9HiEnJYqqVauy\nc+fO9On4+PhzqpX27dvH6tWrad68OeAkz4wN1TNmzGDSpElnzUtMTPR6ffJSkbzrqVSpcDZtOsA/\n/lGSGTO6MXfuHZYkTKFWunRpJkyYwJgxY0hOTuaOO+7g+++/Z8EC5+HRkydP8uCDD/Loo48CMGTI\nEJ5//nk2bdoEOF/cY8eOPWe/HTp0YOLEienTp6ueKlWqxIYNG9KrlrIiItx0000MHjyYBg0apP96\nz7jfVatWnbNtgwYN0u8OAqdEUaVKFQCmT5+e5TE7duzItGnT0ttQdu3axd69e0lISKBs2bJERUWx\ncePGc6p/Ths3bhyrVq065y9jkgC4/vrrmTFjBklJSWzbto3NmzfTrFmzs9YpW7YsCQkJ6dd6/vz5\nNGjQIH35b7/9xqFDh85p19i0adNZVW/+VGQSxbx5W0hKSgGgXLkoZs/uycaND9CjRyN7KMoUCRdf\nfDFNmjRhxowZREZG8sUXXzBq1Cjq1avHRRddxGWXXcaAAQMAaNy4Ma+++iq33XYbDRo0oFGjRuze\nvfucfQ4fPpxDhw7RqFEjmjRpkv5L+8UXX6Rr1660a9eOypUre42rR48evP/+++nVTgATJkwgLi6O\nxo0b07BhQ6ZMmXLOdvXr1ychIYGjR48C8Oijj/L444/TqlUrUlNTszxehw4duP3222nZsiUXXXQR\n3bt35+jRo3Tq1ImUlBQaN27Mk08+eVbbQm5deOGF3HrrrTRs2JBOnToxadKk9Oqjzp078+effxIW\nFsabb75Jt27daNKkCe+99x4vv/xy+j4+/PBDevbsec731KJFi+jSpcvfjtEXklkdWkEWe75o3E7f\nY965M4EHH5zL559v5Nlnr2L48NZ+jM6YMzZs2HDWL0OT98aNG0d0dHSBf5bCH1q3bs0XX3yRabtQ\nZu89EVmpqrG5OVahLVGkpKQxduxSGjSYxOefb6RkyeLExFj338YUJvfffz/h4eGBDiPf7du3j8GD\nB/t080BeKJSN2cuWxdOv35esXr0HgG7dGjB+fCeqVCkV4MiMMXkpIiKCXr16BTqMfFehQgVuvPHG\nfDteoUsUP/0Uz+WXv40q1KhRhokTr6VLlwsCHZYporzdhmqMP/ijOaHQJYpmzarQsWMdLr74Hwwf\n3pqoqLwbvMOYnIiIiODAgQPW1bjJN+qORxEREZGn+w36xuzNmw8waNA8xo7tyAUXOLfWpaUpISH2\nwTSBZSPcmUDIaoS7v9OYHbQliqSkFF588XteeOF7kpJSiYgIY+bMWwEsSZgCoVixYnk6ypgxgeLX\nu55EpJOI/CYiW0TknKdRRCRcRD5yl/8kIjV82e/ChVtp3HgKTz/9HUlJqfTu3ZQpU7rmdfjGGGPw\nY4lCREKBSUB7IB5YISKzVXW9x2p9gEOqWkdEegIvAT3O3dsZ2w6W4Zpr3gOgQYPyTJnS1TrxM8YY\nP/JniaIZsEVVt6rqKWAGcEOGdW4A/s99PRO4WrJp9Tt0MoqIiDCef74dq1b1syRhjDF+5rfGbBHp\nDnRS1Xvd6V5Ac1Ud4LHOr+468e707+46+zPsqy9wumP4RsCvfgk6+JQH9me7VtFg1+IMuxZn2LU4\no56qRme/2rn82ZidWckgY1byZR1U9Q3gDQARictty31hY9fiDLsWZ9i1OMOuxRkiEpfbbf1Z9RQP\nnO8xXRXI2Hl6+joiEgaUBg76MSZjjDE55M9EsQKoKyI1RaQ40BOYnWGd2cDd7uvuwLcabA92GGNM\nIee3qidVTRGRAcA8IBSYpqrrRGQkziDfs4G3gfdEZAtOSaKnD7t+w18xByG7FmfYtTjDrsUZdi3O\nyPW1CLons40xxuSvQtvNuDHGmLxhicIYY4xXBTZR+Kv7j2Dkw7UYLCLrRWSNiCwUkUL7FGJ218Jj\nve4ioiJSaG+N9OVaiMit7ntjnYh8kN8x5hcfPiPVRGSRiPzifk46ByJOfxORaSKy131GLbPlIiIT\n3Ou0RkQu8WnHqlrg/nAav38HagHFgdVAwwzr9AemuK97Ah8FOu4AXourgCj39f1F+Vq460UDS4Bl\nQGyg4w7g+6Iu8AtQ1p2uGOi4A3gt3gDud183BLYHOm4/XYvWwCXAr1ks7wx8jfMMWwvgJ1/2W1BL\nFH7p/iNIZXstVHWRqp5wJ5fhPLNSGPnyvgB4FhgNFOb+vX25Fv8CJqnqIQBV3ZvPMeYXX66FAqeH\nuCzNuc90FQqqugTvz6LdALyrjmVAGRGpnN1+C2qiqALs9JiOd+dluo6qpgAJQLl8iS5/+XItPPXB\n+cVQGGV7LUTkYuB8Vf0yPwMLAF/eFxcAF4jIDyKyTEQ65Vt0+cuXa/E0cKeIxANzgIH5E1qBk9Pv\nE6DgjkeRZ91/FAI+n6eI3AnEAm38GlHgeL0WIhICjAPuya+AAsiX90UYTvVTW5xS5v9EpJGqHvZz\nbPnNl2txGzBdVV8RkZY4z281UtU0/4dXoOTqe7Ogliis+48zfLkWiMg1wDDgelVNyqfY8lt21yIa\np9PIxSKyHacOdnYhbdD29TPyhaomq+o24DecxFHY+HIt+gAfA6jqUiACp8PAosan75OMCmqisO4/\nzsj2WrjVLVNxkkRhrYeGbK6FqiaoanlVraGqNXDaa65X1Vx3hlaA+fIZ+RznRgdEpDxOVdTWfI0y\nf/hyLXYAVwOISAOcRLEvX6MsGGYDd7l3P7UAElR1d3YbFciqJ/Vf9x9Bx8dr8TJQEvjEbc/foarX\nByxoP/HxWhQJPl6LeUAHEVkPpAJDVPVA4KL2Dx+vxSPAmyIyCKeq5Z7C+MNSRD7EqWos77bHjACK\nAajqFJz2mc7AFuAE0Nun/RbCa2WMMSYPFdSqJ2OMMQWEJQpjjDFeWaIwxhjjlSUKY4wxXlmiMMYY\n45UlClPgiEiqiKzy+KvhZd0aWfWUmcNjLnZ7H13tdnlRLxf76Ccid7mv7xGR8zyWvSUiDfM4zhUi\n0tSHbR4Wkai/e2xTdFmiMAXRSVVt6vG3PZ+Oe4eqNsHpbPLlnG6sqlNU9V138h7gPI9l96rq+jyJ\n8kycr+NbnA8DlihMrlmiMEHBLTn8T0R+dv8uz2SdC0VkuVsKWSMidd35d3rMnyoiodkcbglQx932\nancMg7VuX//h7vwX5cwYIGPceU+LyL9FpDtOn1v/cY8Z6ZYEYkXkfhEZ7RHzPSLyWi7jXIpHh24i\nMllE4sQZe+IZd96DOAlrkYgscud1EJGl7nX8RERKZnMcU8RZojAFUaRHtdMsd95eoL2qXgL0ACZk\nsl0/YLyqNsX5oo53u2voAbRy56cCd2Rz/OuAtSISAUwHeqjqRTg9GdwvIjHATcCFqtoYGOW5sarO\nBOJwfvk3VdWTHotnAjd7TPcAPsplnJ1wuuk4bZiqxgKNgTYi0lhVJ+D05XOVql7lduUxHLjGvZZx\nwOBsjmOKuALZhYcp8k66X5aeigET3Tr5VJx+izJaCgwTkarAZ6q6WUSuBi4FVrjdm0TiJJ3M/EdE\nTgLbcbqhrgdsU9VN7vL/Ax4AJuKMdfGWiHwF+NyluaruE5Gtbj87m91j/ODuNydxlsDprsJzhLJb\nRaQvzue6Ms4APWsybNvCnf+De5ziONfNmCxZojDBYhCwB2iCUxI+Z1AiVf1ARH4CugDzRORenG6V\n/09VH/fhGHd4diAoIpmOb+L2LdQMp5O5nsAAoF0OzuUj4FZgIzBLVVWcb22f48QZxe1FYBJws4jU\nBP4NXKaqh0RkOk7HdxkJMF9Vb8tBvKaIs6onEyxKA7vd8QN64fyaPouI1AK2utUts3GqYBYC3UWk\nortOjPg+pvhGoIaI1HGnewHfuXX6pVV1Dk5DcWZ3Hh3F6fY8M58BN+KMkfCROy9HcapqMk4VUgu3\n2qoUcBxIEJFKwLVZxLIMaHX6nEQkSkQyK50Zk84ShQkWrwN3i8gynGqn45ms0wP4VURWAfVxhnxc\nj/OF+o2IrAHm41TLZEtVE3F61/xERNYCacAUnC/dL939fYdT2sloOjDldGN2hv0eAtYD1VV1uTsv\nx3G6bR+vAP9W1dU442OvA6bhVGed9gbwtYgsUtV9OHdkfegeZxnOtTImS9Z7rDHGGK+sRGGMMcYr\nSxTGGGO8skRhjDHGK0sUxhhjvLJEYYwxxitLFMYYY7yyRGGMMcar/wemWTRHkq8rRAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ae00b38a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = convert_to_two(y_real)\n",
    "fpr, tpr, _ = roc_curve(y_true.ravel(), preds.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "fig2 = plt.gcf()\n",
    "plt.show()\n",
    "fig2.savefig(\"ROC.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_roc_curve(model, val_dir, fname):\n",
    "    \n",
    "    y_pred = predict(val_dir, model)\n",
    "    \n",
    "    y_real = get_labels(val_dir)\n",
    "\n",
    "    y_true = convert_to_two(y_real)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true.ravel(), preds.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    fig2 = plt.gcf()\n",
    "    plt.show()\n",
    "    fig2.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "get_roc_curve('resnet_2048_16_none.h5', 'val_CAL', 'roc_16.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
